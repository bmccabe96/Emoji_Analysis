{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello this has been KARINA and WINTER We re cu...</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ilysm you deserve the world please you re awes...</td>\n",
       "      <td>0.7284</td>\n",
       "      <td>üòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey 965TDY I d like to hear Naughty List by li...</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I were to lift her up I d die SingleAndMing...</td>\n",
       "      <td>0.9723</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Blair don t be a lier a Blier R G Mugabe RI...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>üòÇ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Hello this has been KARINA and WINTER We re cu...           0.9840   \n",
       "1  ilysm you deserve the world please you re awes...           0.7284   \n",
       "2  Hey 965TDY I d like to hear Naughty List by li...           0.9168   \n",
       "3  If I were to lift her up I d die SingleAndMing...           0.9723   \n",
       "4  Mr Blair don t be a lier a Blier R G Mugabe RI...           0.4404   \n",
       "\n",
       "  top_emoji  \n",
       "0         üòä  \n",
       "1         üòç  \n",
       "2         üòä  \n",
       "3         üòÇ  \n",
       "4         üòÇ  "
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_7_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this has been KARINA and WINTER We re currently looking for our dearest member t https t co w9NXwy7G8b\n",
      "hello karina winter currently looking dearest member\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40462.000000\n",
       "mean         0.608406\n",
       "std          0.306372\n",
       "min          0.000000\n",
       "25%          0.329932\n",
       "50%          0.720230\n",
       "75%          0.885921\n",
       "max          1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40462/40462 [00:04<00:00, 9991.94it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello this has been KARINA and WINTER We re cu...</td>\n",
       "      <td>0.992098</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ilysm you deserve the world please you re awes...</td>\n",
       "      <td>0.864266</td>\n",
       "      <td>üòç</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey 965TDY I d like to hear Naughty List by li...</td>\n",
       "      <td>0.958490</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I were to lift her up I d die SingleAndMing...</td>\n",
       "      <td>0.986247</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Blair don t be a lier a Blier R G Mugabe RI...</td>\n",
       "      <td>0.720230</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Hello this has been KARINA and WINTER We re cu...         0.992098   \n",
       "1  ilysm you deserve the world please you re awes...         0.864266   \n",
       "2  Hey 965TDY I d like to hear Naughty List by li...         0.958490   \n",
       "3  If I were to lift her up I d die SingleAndMing...         0.986247   \n",
       "4  Mr Blair don t be a lier a Blier R G Mugabe RI...         0.720230   \n",
       "\n",
       "  top_emoji  capitalization  \n",
       "0         üòä        0.100000  \n",
       "1         üòç        0.000000  \n",
       "2         üòä        0.133333  \n",
       "3         üòÇ        0.133333  \n",
       "4         üòÇ        0.176471  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40462/40462 [01:18<00:00, 513.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello this has been KARINA and WINTER We re cu...</td>\n",
       "      <td>0.992098</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ilysm you deserve the world please you re awes...</td>\n",
       "      <td>0.864266</td>\n",
       "      <td>üòç</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey 965TDY I d like to hear Naughty List by li...</td>\n",
       "      <td>0.958490</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If I were to lift her up I d die SingleAndMing...</td>\n",
       "      <td>0.986247</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Blair don t be a lier a Blier R G Mugabe RI...</td>\n",
       "      <td>0.720230</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Hello this has been KARINA and WINTER We re cu...         0.992098   \n",
       "1  ilysm you deserve the world please you re awes...         0.864266   \n",
       "2  Hey 965TDY I d like to hear Naughty List by li...         0.958490   \n",
       "3  If I were to lift her up I d die SingleAndMing...         0.986247   \n",
       "4  Mr Blair don t be a lier a Blier R G Mugabe RI...         0.720230   \n",
       "\n",
       "  top_emoji  capitalization  profanity  \n",
       "0         üòä        0.100000   0.000000  \n",
       "1         üòç        0.000000   0.000000  \n",
       "2         üòä        0.133333   0.000000  \n",
       "3         üòÇ        0.133333   0.014286  \n",
       "4         üòÇ        0.176471   0.000000  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions = [\n",
    "#     df.top_emoji == 'üòÇ',\n",
    "#     df.top_emoji == 'üò≠',\n",
    "#     df.top_emoji == 'üòç',\n",
    "#     df.top_emoji == 'ü§î',\n",
    "#     df.top_emoji == 'üòä',\n",
    "#     df.top_emoji == 'üò±',\n",
    "#     df.top_emoji == 'üò°'\n",
    "# ]\n",
    "# choices = [\n",
    "#     '0','1',2,3,4,5,6\n",
    "# ]\n",
    "# df['top_emoji'] = np.select(conditions, choices, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('top_emoji', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21615342790766645\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier()\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.21659828975334883)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòÇ    13949\n",
       "üò≠     8719\n",
       "üòç     7192\n",
       "ü§î     3740\n",
       "üòä     3230\n",
       "üò±     2477\n",
       "üò°     1155\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-291-9e4e0024c594>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-291-9e4e0024c594>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    len(anger).*1.5\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "len(anger).*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üò°    1732\n",
       "üòä    1732\n",
       "üò±    1732\n",
       "ü§î    1732\n",
       "üò≠    1732\n",
       "üòÇ    1732\n",
       "üòç    1732\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "laugh = df[df.top_emoji == 'üòÇ']\n",
    "cry = df[df.top_emoji == 'üò≠']\n",
    "love = df[df.top_emoji == 'üòç']\n",
    "wonder = df[df.top_emoji == 'ü§î']\n",
    "happy = df[df.top_emoji == 'üòä']\n",
    "fear = df[df.top_emoji == 'üò±']\n",
    "anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "laugh_downsampled = resample(laugh,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = int(len(anger)*1.5), # match minority n\n",
    "                                random_state = 40) # reproducible results\n",
    "\n",
    "cry_downsampled = resample(cry,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "love_downsampled = resample(love,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "wonder_downsampled = resample(wonder,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "happy_downsampled = resample(happy,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "fear_downsampled = resample(fear,\n",
    "                          replace=False, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "anger_upsampled = resample(anger,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(anger)*1.5), # match number in majority class\n",
    "                          random_state=40) \n",
    "\n",
    "df = pd.concat([laugh_downsampled, cry_downsampled, love_downsampled, wonder_downsampled, happy_downsampled, \n",
    "                fear_downsampled, anger_upsampled])\n",
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'], 'sent': row['sentiment_score']} for _, row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization', 'profanity', 'sentiment_score'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 0.9,\n",
    "            'stats': 1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40\n",
    "X = df[['tweet', 'capitalization', 'profanity', 'sentiment_score']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.top_emoji, test_size=0.2, random_state=seed, \n",
    "                                                    stratify=df.top_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   3.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               10),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x1b2d3bdc0>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1.5,\n",
       "                                                   'text': 0.9}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (9699, 10100) - (2425, 10100)\n",
      "CPU times: user 4.28 s, sys: 28.2 ms, total: 4.31 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=40,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.5023\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('SVM', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòÇ       0.43      0.49      0.46       347\n",
      "           üòä       0.39      0.41      0.40       346\n",
      "           üòç       0.40      0.37      0.38       346\n",
      "           üò°       0.71      0.71      0.71       347\n",
      "           üò≠       0.51      0.58      0.54       346\n",
      "           üò±       0.68      0.66      0.67       346\n",
      "           ü§î       0.38      0.29      0.33       347\n",
      "\n",
      "    accuracy                           0.50      2425\n",
      "   macro avg       0.50      0.50      0.50      2425\n",
      "weighted avg       0.50      0.50      0.50      2425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=200,random_state=40,n_jobs=-1,verbose=1)\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.5897\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòÇ       0.53      0.56      0.55       347\n",
      "           üòä       0.51      0.54      0.53       346\n",
      "           üòç       0.46      0.38      0.41       346\n",
      "           üò°       0.84      0.76      0.79       347\n",
      "           üò≠       0.58      0.68      0.63       346\n",
      "           üò±       0.71      0.74      0.73       346\n",
      "           ü§î       0.50      0.47      0.49       347\n",
      "\n",
      "    accuracy                           0.59      2425\n",
      "   macro avg       0.59      0.59      0.59      2425\n",
      "weighted avg       0.59      0.59      0.59      2425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[195  70  27   5  10  18  22]\n",
      " [ 74 186  59   2   6   6  13]\n",
      " [ 55  71 130   8  18  15  49]\n",
      " [  5   4   9 262  23   6  38]\n",
      " [  7   7  20  16 237  30  29]\n",
      " [  1   3   9   5  59 256  13]\n",
      " [ 29  21  31  15  59  28 164]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.3946\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.3233\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.21659828975334883),\n",
       " ('SVM', 0.5022680412371134),\n",
       " ('RFC', 0.5896907216494846),\n",
       " ('MNBayes', 0.39463917525773196),\n",
       " ('BerBayes', 0.32329896907216493)]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.4619\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.21659828975334883),\n",
       " ('SVM', 0.5022680412371134),\n",
       " ('RFC', 0.5896907216494846),\n",
       " ('MNBayes', 0.39463917525773196),\n",
       " ('BerBayes', 0.32329896907216493),\n",
       " ('PassiveAgg', 0.4618556701030928)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results with Selection Biased Data\", fontsize=20)\n",
    "plt.savefig(\"/Users/brianmccabe/Desktop/Early_Results_Selection_Biased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = ['I love going to the beach',\n",
    "              'I hate you',\n",
    "              'Trump makes me so mad when he talks like that',\n",
    "              'I am scared of the results',\n",
    "              \"I don't understand how this works\",\n",
    "              \"You are amazing thank you for reaching out\",\n",
    "              \"Why do people do that, it is just aweful\",\n",
    "              \"That is so sad\"]\n",
    "model_testing = pd.DataFrame({'tweet': [], 'capitalization': [], 'profanity':[], 'sentiment_score':[]})\n",
    "to_add = []\n",
    "for tweet in test_tweets:\n",
    "    row = {'tweet': tweet,\n",
    "             'capitalization': capital_percentage(tweet),\n",
    "             'profanity': check_profanity(tweet),\n",
    "          'sentiment_score': return_sentiment(tweet)}\n",
    "    to_add.append(row)\n",
    "\n",
    "model_testing = model_testing.append(to_add) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.000000\n",
       "mean     0.383160\n",
       "std      0.383635\n",
       "min      0.000000\n",
       "25%      0.098050\n",
       "50%      0.297471\n",
       "75%      0.547364\n",
       "max      1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "model_testing.sentiment_score = normalizer.fit_transform(np.array(model_testing.sentiment_score).reshape(-1,1))\n",
    "model_testing.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love going to the beach</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate you</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump makes me so mad when he talks like that</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.160688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am scared of the results</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't understand how this works</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You are amazing thank you for reaching out</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why do people do that, it is just aweful</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>That is so sad</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet  capitalization  profanity  \\\n",
       "0                      I love going to the beach        0.166667   0.000000   \n",
       "1                                     I hate you        0.333333   0.000000   \n",
       "2  Trump makes me so mad when he talks like that        0.000000   0.022222   \n",
       "3                     I am scared of the results        0.166667   0.000000   \n",
       "4              I don't understand how this works        0.142857   0.000000   \n",
       "5     You are amazing thank you for reaching out        0.000000   0.000000   \n",
       "6       Why do people do that, it is just aweful        0.000000   0.000000   \n",
       "7                                 That is so sad        0.000000   0.000000   \n",
       "\n",
       "   sentiment_score  \n",
       "0         0.886695  \n",
       "1         0.027989  \n",
       "2         0.160688  \n",
       "3         0.121404  \n",
       "4         0.434254  \n",
       "5         1.000000  \n",
       "6         0.434254  \n",
       "7         0.000000  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 4.88 ms, total: 17.4 ms\n",
      "Wall time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_testing_vec = pipeline.transform(model_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x356246 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_testing_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing_preds = sv_clf.predict(model_testing_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 4, 0, 2, 6, 4])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_testing_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
