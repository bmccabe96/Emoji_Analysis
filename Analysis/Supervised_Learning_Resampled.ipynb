{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "2  omg there s more on the ballot then just the p...          -0.7003   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...           0.5437   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         üò©  \n",
       "1            0.010101         üòä  \n",
       "2            0.000000         üò±  \n",
       "3            0.000000         üòä  \n",
       "4            0.021429         üòä  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3654.000000\n",
       "mean        0.591639\n",
       "std         0.309474\n",
       "min         0.000000\n",
       "25%         0.290590\n",
       "50%         0.690462\n",
       "75%         0.871406\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3654/3654 [00:00<00:00, 7713.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         üò©        0.000000  \n",
       "1            0.010101         üòä        0.111111  \n",
       "2            0.000000         üò±        0.000000  \n",
       "3            0.000000         üòä        0.000000  \n",
       "4            0.021429         üòä        0.037037  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3654/3654 [00:08<00:00, 453.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         üò©        0.000000   0.000000  \n",
       "1            0.010101         üòä        0.111111   0.010753  \n",
       "2            0.000000         üò±        0.000000   0.000000  \n",
       "3            0.000000         üòä        0.000000   0.000000  \n",
       "4            0.021429         üòä        0.037037   0.007576  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3654/3654 [00:00<00:00, 4995.11it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's!! gooooo to the zoooo!. Wazzzzuppppp!!!. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's!! goo to the zoo!. Wazzupp!!. AABBCC\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    2058\n",
       "üò©     831\n",
       "üò°     517\n",
       "üò±     248\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a very clear and large class imbalance. In this notebook, all models try to handle this inbalance by balancing the weights given to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2441160372194855\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    2058\n",
       "üò©     831\n",
       "üò°     517\n",
       "üò±     248\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Data to address Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    372\n",
       "üò°    372\n",
       "üò©    372\n",
       "üò±    372\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "cry = df[df.top_emoji == 'üò©']\n",
    "happy = df[df.top_emoji == 'üòä']\n",
    "fear = df[df.top_emoji == 'üò±']\n",
    "anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "cry_downsampled = resample(cry,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number\n",
    "                          random_state=seed) \n",
    "\n",
    "happy_downsampled = resample(happy,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "fear_upsampled = resample(fear,\n",
    "                          replace=True, \n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "anger_upsampled = resample(anger,\n",
    "                          replace=True,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "\n",
    "df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        rs = []\n",
    "        for row in data.iterrows():\n",
    "            to_add = {}\n",
    "            for item in row[1:]:\n",
    "                for ind, val in zip(item.index, item.values):\n",
    "                    to_add[ind] = val\n",
    "            rs.append(to_add)\n",
    "        return rs\n",
    "#         return [{'cap':  row['capitalization'], 'prof': row['profanity'], \n",
    "#                  'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ItemSelector(['capitalization','sentiment_score','exclamation_points']).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = []\n",
    "for row in test.transform(X).iterrows():\n",
    "    to_add = {}\n",
    "    for item in row[1:]:\n",
    "        for ind, val in zip(item.index, item.values):\n",
    "            to_add[ind] = val\n",
    "    rs.append(to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'capitalization': 0.13333333333333333,\n",
       "  'sentiment_score': 0.22753231139164415,\n",
       "  'exclamation_points': 0.0},\n",
       " {'capitalization': 0.125,\n",
       "  'sentiment_score': 0.3858330828574291,\n",
       "  'exclamation_points': 0.0}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to help in the creation of our training arrays to include custom features as well as TF IDF and Word Embeddings.\n",
    "\n",
    "- NOTE: Word embeddings hurt the score, hence it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(input_selectors): \n",
    "    pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "\n",
    "                # Pipeline for pulling features from the text\n",
    "                ('text', Pipeline([\n",
    "                    ('selector', ItemSelector(key='tweet')),\n",
    "                    ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                        stop_words = None, preprocessor=clean_txt)),\n",
    "                ])),\n",
    "\n",
    "    #             ('embedding', Pipeline([\n",
    "    #                 ('selector', ItemSelector(key='tweet')),\n",
    "    #                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "    #             ])),\n",
    "\n",
    "                # Pipeline for pulling metadata features\n",
    "                ('stats', Pipeline([\n",
    "                    ('selector', ItemSelector(key=input_selectors)),\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ])),\n",
    "\n",
    "            ],\n",
    "\n",
    "            # weight components in FeatureUnion\n",
    "            transformer_weights={\n",
    "                'text': 1,#0.9,\n",
    "    #             'embedding': 1,\n",
    "                'stats': 1 #1.5,\n",
    "            },\n",
    "        ))\n",
    "    ], verbose=True)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['capitalization','profanity','sentiment_score','exclamation_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cap = create_pipeline(['capitalization'])\n",
    "pipeline_prof = create_pipeline(['profanity'])\n",
    "pipeline_sent = create_pipeline(['sentiment_score'])\n",
    "pipeline_excl = create_pipeline(['exclamation_points'])\n",
    "pipeline_all = create_pipeline(['capitalization','profanity','sentiment_score','exclamation_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.4s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.4s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.4s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.4s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x125cc6040>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cap.fit(X_train)\n",
    "pipeline_prof.fit(X_train)\n",
    "pipeline_sent.fit(X_train)\n",
    "pipeline_excl.fit(X_train)\n",
    "pipeline_all.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (1190, 1243) - (298, 1243)\n",
      "Checking that the shapes match: (1190, 1243) - (298, 1243)\n",
      "Checking that the shapes match: (1190, 1243) - (298, 1243)\n",
      "Checking that the shapes match: (1190, 1243) - (298, 1243)\n",
      "Checking that the shapes match: (1190, 1246) - (298, 1246)\n",
      "CPU times: user 2.47 s, sys: 18.9 ms, total: 2.49 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec_cap = pipeline_cap.transform(X_train)\n",
    "test_vec_cap = pipeline_cap.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_cap.shape, test_vec_cap.shape))\n",
    "\n",
    "train_vec_prof = pipeline_prof.transform(X_train)\n",
    "test_vec_prof = pipeline_prof.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_prof.shape, test_vec_prof.shape))\n",
    "\n",
    "train_vec_sent = pipeline_sent.transform(X_train)\n",
    "test_vec_sent = pipeline_sent.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_sent.shape, test_vec_sent.shape))\n",
    "\n",
    "train_vec_excl = pipeline_excl.transform(X_train)\n",
    "test_vec_excl = pipeline_excl.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_excl.shape, test_vec_excl.shape))\n",
    "\n",
    "train_vec_all = pipeline_all.transform(X_train)\n",
    "test_vec_all = pipeline_all.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_all.shape, test_vec_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = [train_vec_cap, train_vec_prof, train_vec_sent, train_vec_excl, train_vec_all]\n",
    "tests = [test_vec_cap, test_vec_prof, test_vec_sent, test_vec_excl, test_vec_all]\n",
    "labels = ['capitalization', 'profanity', 'sentiment_score', 'exclamation_points', 'all_custom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overarching Function for Iterative Modeling\n",
    "- Takes in a model\n",
    "- Runs that model against all the different train/test vecs with dif features\n",
    "- prints out accuracy and description of what train/test vec worked best\n",
    "- prints out confusion matrix and classification report for best version of the model\n",
    "- Returns best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, model_name):\n",
    "    res = []\n",
    "    for train, test, label in zip(trains, tests, labels):\n",
    "        if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "            model.fit(train, y_train, eval_metric='mlogloss')\n",
    "        else:\n",
    "            model.fit(train, y_train)\n",
    "        test_preds = model.predict(test)\n",
    "        accuracy = accuracy_score(y_test, test_preds)\n",
    "        res.append({'label': label, 'score': accuracy, 'test_preds': test_preds})\n",
    "    res = sorted(res, key = lambda x: x['score'], reverse=True)\n",
    "    print('RESULTS')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(f\"{model_name} with {res[0]['label']} features performed the best with an accuracy of {res[0]['score']}\")\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, res[0]['test_preds']))\n",
    "    print('----------------------------------------')\n",
    "    print(confusion_matrix(y_test, res[0]['test_preds']))\n",
    "    return res[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "log reg with sentiment_score features performed the best with an accuracy of 0.714765100671141\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.74      0.85      0.80        75\n",
      "           üò°       0.68      0.68      0.68        74\n",
      "           üò©       0.63      0.55      0.59        75\n",
      "           üò±       0.79      0.78      0.79        74\n",
      "\n",
      "    accuracy                           0.71       298\n",
      "   macro avg       0.71      0.71      0.71       298\n",
      "weighted avg       0.71      0.71      0.71       298\n",
      "\n",
      "----------------------------------------\n",
      "[[64  6  4  1]\n",
      " [ 9 50 11  4]\n",
      " [ 9 15 41 10]\n",
      " [ 4  3  9 58]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "results.append(('log reg', get_model_results(lr_clf, 'log reg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446), ('log reg', 0.714765100671141)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "linear svc with all_custom features performed the best with an accuracy of 0.6946308724832215\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.74      0.85      0.79        75\n",
      "           üò°       0.64      0.64      0.64        74\n",
      "           üò©       0.60      0.55      0.57        75\n",
      "           üò±       0.79      0.74      0.76        74\n",
      "\n",
      "    accuracy                           0.69       298\n",
      "   macro avg       0.69      0.69      0.69       298\n",
      "weighted avg       0.69      0.69      0.69       298\n",
      "\n",
      "----------------------------------------\n",
      "[[64  4  5  2]\n",
      " [ 9 47 12  6]\n",
      " [11 16 41  7]\n",
      " [ 3  6 10 55]]\n"
     ]
    }
   ],
   "source": [
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed) \n",
    "results.append(('linear svc', get_model_results(sv_clf, 'linear svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "svc with all_custom features performed the best with an accuracy of 0.7583892617449665\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.78      0.89      0.83        75\n",
      "           üò°       0.77      0.65      0.71        74\n",
      "           üò©       0.68      0.67      0.67        75\n",
      "           üò±       0.80      0.82      0.81        74\n",
      "\n",
      "    accuracy                           0.76       298\n",
      "   macro avg       0.76      0.76      0.76       298\n",
      "weighted avg       0.76      0.76      0.76       298\n",
      "\n",
      "----------------------------------------\n",
      "[[67  3  5  0]\n",
      " [ 8 48 13  5]\n",
      " [ 7  8 50 10]\n",
      " [ 4  3  6 61]]\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=15, class_weight='balanced', kernel='rbf', gamma='scale')\n",
    "results.append(('svc', get_model_results(svc, 'svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "rfc with all_custom features performed the best with an accuracy of 0.7684563758389261\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.74      0.93      0.82        75\n",
      "           üò°       0.79      0.72      0.75        74\n",
      "           üò©       0.72      0.61      0.66        75\n",
      "           üò±       0.83      0.81      0.82        74\n",
      "\n",
      "    accuracy                           0.77       298\n",
      "   macro avg       0.77      0.77      0.76       298\n",
      "weighted avg       0.77      0.77      0.76       298\n",
      "\n",
      "----------------------------------------\n",
      "[[70  0  4  1]\n",
      " [11 53  6  4]\n",
      " [10 12 46  7]\n",
      " [ 4  2  8 60]]\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=400, random_state=seed,n_jobs=-1,\n",
    "                                 class_weight='balanced',criterion='entropy' )\n",
    "results.append(('rfc', get_model_results(rfc_clf, 'rfc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('rfc', 0.7684563758389261)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "mn bayes with all_custom features performed the best with an accuracy of 0.5503355704697986\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.46      0.84      0.59        75\n",
      "           üò°       0.60      0.42      0.49        74\n",
      "           üò©       0.48      0.41      0.45        75\n",
      "           üò±       0.87      0.53      0.66        74\n",
      "\n",
      "    accuracy                           0.55       298\n",
      "   macro avg       0.60      0.55      0.55       298\n",
      "weighted avg       0.60      0.55      0.55       298\n",
      "\n",
      "----------------------------------------\n",
      "[[63  1 10  1]\n",
      " [23 31 16  4]\n",
      " [29 14 31  1]\n",
      " [22  6  7 39]]\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "results.append(('mn bayes', get_model_results(mnb_clf, 'mn bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('rfc', 0.7684563758389261),\n",
       " ('mn bayes', 0.5503355704697986)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "bernoulli bayes with profanity features performed the best with an accuracy of 0.4697986577181208\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.40      0.64      0.49        75\n",
      "           üò°       0.74      0.35      0.48        74\n",
      "           üò©       0.34      0.53      0.42        75\n",
      "           üò±       0.96      0.35      0.51        74\n",
      "\n",
      "    accuracy                           0.47       298\n",
      "   macro avg       0.61      0.47      0.48       298\n",
      "weighted avg       0.61      0.47      0.48       298\n",
      "\n",
      "----------------------------------------\n",
      "[[48  1 26  0]\n",
      " [18 26 29  1]\n",
      " [30  5 40  0]\n",
      " [24  3 21 26]]\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "results.append(('bernoulli bayes', get_model_results(bb_clf, 'bernoulli bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('rfc', 0.7684563758389261),\n",
       " ('mn bayes', 0.5503355704697986),\n",
       " ('bernoulli bayes', 0.4697986577181208)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "pac with all_custom features performed the best with an accuracy of 0.7046979865771812\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.75      0.83      0.78        75\n",
      "           üò°       0.63      0.73      0.68        74\n",
      "           üò©       0.63      0.53      0.58        75\n",
      "           üò±       0.82      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.70       298\n",
      "   macro avg       0.71      0.70      0.70       298\n",
      "weighted avg       0.71      0.70      0.70       298\n",
      "\n",
      "----------------------------------------\n",
      "[[62  9  3  1]\n",
      " [ 9 54 10  1]\n",
      " [12 13 40 10]\n",
      " [ 0 10 10 54]]\n"
     ]
    }
   ],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "results.append(('pac', get_model_results(pac_clf, 'pac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('rfc', 0.7684563758389261),\n",
       " ('mn bayes', 0.5503355704697986),\n",
       " ('bernoulli bayes', 0.4697986577181208),\n",
       " ('pac', 0.7046979865771812)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "xgboost with sentiment_score features performed the best with an accuracy of 0.7449664429530202\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.79      0.88      0.83        75\n",
      "           üò°       0.75      0.70      0.73        74\n",
      "           üò©       0.66      0.59      0.62        75\n",
      "           üò±       0.77      0.81      0.79        74\n",
      "\n",
      "    accuracy                           0.74       298\n",
      "   macro avg       0.74      0.75      0.74       298\n",
      "weighted avg       0.74      0.74      0.74       298\n",
      "\n",
      "----------------------------------------\n",
      "[[66  4  5  0]\n",
      " [ 9 52 10  3]\n",
      " [ 5 11 44 15]\n",
      " [ 4  2  8 60]]\n"
     ]
    }
   ],
   "source": [
    "xg = xgb.XGBClassifier(n_jobs=-1)\n",
    "results.append(('xgboost', get_model_results(xg, 'xgboost')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24548440065681446),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('rfc', 0.7684563758389261),\n",
       " ('mn bayes', 0.5503355704697986),\n",
       " ('bernoulli bayes', 0.4697986577181208),\n",
       " ('pac', 0.7046979865771812),\n",
       " ('xgboost', 0.7449664429530202)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "voting with all_custom features performed the best with an accuracy of 0.7348993288590604\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.75      0.91      0.82        75\n",
      "           üò°       0.71      0.68      0.69        74\n",
      "           üò©       0.62      0.60      0.61        75\n",
      "           üò±       0.86      0.76      0.81        74\n",
      "\n",
      "    accuracy                           0.73       298\n",
      "   macro avg       0.74      0.73      0.73       298\n",
      "weighted avg       0.74      0.73      0.73       298\n",
      "\n",
      "----------------------------------------\n",
      "[[68  2  5  0]\n",
      " [ 9 50 12  3]\n",
      " [10 14 45  6]\n",
      " [ 4  4 10 56]]\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('svc', svc), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #(\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "results.append(('voting', get_model_results(voting_clf, 'voting')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rfc', 0.7684563758389261),\n",
       " ('svc', 0.7583892617449665),\n",
       " ('xgboost', 0.7449664429530202),\n",
       " ('voting', 0.7348993288590604),\n",
       " ('log reg', 0.714765100671141),\n",
       " ('pac', 0.7046979865771812),\n",
       " ('linear svc', 0.6946308724832215),\n",
       " ('mn bayes', 0.5503355704697986),\n",
       " ('bernoulli bayes', 0.4697986577181208),\n",
       " ('Dummy', 0.24548440065681446)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Model Results\", fontsize=20)\n",
    "plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
