{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repost USElection2020 Trump and Covid Jimmy Ki...</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ripdonaldtrump gone and always forgotten have ...</td>\n",
       "      <td>-0.7430</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scummo election visit caused an 11 swing to La...</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titan Medium Lovem Martis Titan Zeus Mars Vote...</td>\n",
       "      <td>0.4587</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Nashies y all know who to vote for @CharlesEst...           0.7424   \n",
       "1  Repost USElection2020 Trump and Covid Jimmy Ki...           0.9231   \n",
       "2  ripdonaldtrump gone and always forgotten have ...          -0.7430   \n",
       "3  Scummo election visit caused an 11 swing to La...           0.9260   \n",
       "4  Titan Medium Lovem Martis Titan Zeus Mars Vote...           0.4587   \n",
       "\n",
       "  top_emoji  \n",
       "0         üòä  \n",
       "1         üòä  \n",
       "2         üò≠  \n",
       "3         üòä  \n",
       "4         üòä  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nashies y all know who to vote for @CharlesEsten https t co yn5KOEYOgY\n",
      "nashies know vote\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1025.000000\n",
       "mean        0.522611\n",
       "std         0.319214\n",
       "min         0.000000\n",
       "25%         0.229706\n",
       "50%         0.500427\n",
       "75%         0.843327\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1025/1025 [00:00<00:00, 7111.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.873586</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repost USElection2020 Trump and Covid Jimmy Ki...</td>\n",
       "      <td>0.964413</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ripdonaldtrump gone and always forgotten have ...</td>\n",
       "      <td>0.126967</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scummo election visit caused an 11 swing to La...</td>\n",
       "      <td>0.965871</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titan Medium Lovem Martis Titan Zeus Mars Vote...</td>\n",
       "      <td>0.730988</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Nashies y all know who to vote for @CharlesEst...         0.873586   \n",
       "1  Repost USElection2020 Trump and Covid Jimmy Ki...         0.964413   \n",
       "2  ripdonaldtrump gone and always forgotten have ...         0.126967   \n",
       "3  Scummo election visit caused an 11 swing to La...         0.965871   \n",
       "4  Titan Medium Lovem Martis Titan Zeus Mars Vote...         0.730988   \n",
       "\n",
       "  top_emoji  capitalization  \n",
       "0         üòä             0.0  \n",
       "1         üòä             0.0  \n",
       "2         üò≠             0.0  \n",
       "3         üòä             0.0  \n",
       "4         üòä             0.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1025/1025 [00:02<00:00, 477.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.873586</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repost USElection2020 Trump and Covid Jimmy Ki...</td>\n",
       "      <td>0.964413</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ripdonaldtrump gone and always forgotten have ...</td>\n",
       "      <td>0.126967</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scummo election visit caused an 11 swing to La...</td>\n",
       "      <td>0.965871</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titan Medium Lovem Martis Titan Zeus Mars Vote...</td>\n",
       "      <td>0.730988</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  Nashies y all know who to vote for @CharlesEst...         0.873586   \n",
       "1  Repost USElection2020 Trump and Covid Jimmy Ki...         0.964413   \n",
       "2  ripdonaldtrump gone and always forgotten have ...         0.126967   \n",
       "3  Scummo election visit caused an 11 swing to La...         0.965871   \n",
       "4  Titan Medium Lovem Martis Titan Zeus Mars Vote...         0.730988   \n",
       "\n",
       "  top_emoji  capitalization  profanity  \n",
       "0         üòä             0.0   0.000000  \n",
       "1         üòä             0.0   0.000000  \n",
       "2         üò≠             0.0   0.013158  \n",
       "3         üòä             0.0   0.000000  \n",
       "4         üòä             0.0   0.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üò≠    418\n",
       "üòä    381\n",
       "üò±    125\n",
       "üò°    101\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32195121951219513\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier()\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.3297560975609756)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'], 'sent': row['sentiment_score']} for _, row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization', 'profanity', 'sentiment_score'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 0.9,\n",
    "            'stats': 1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 40\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               10),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x1188f9af0>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1.5,\n",
       "                                                   'text': 0.9}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (820, 943) - (205, 943)\n",
      "CPU times: user 388 ms, sys: 6.45 ms, total: 394 ms\n",
      "Wall time: 395 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.722\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('SVM', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=200,random_state=seed,n_jobs=-1,verbose=1)\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7122\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.5854\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.4244\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.3297560975609756),\n",
       " ('SVM', 0.7219512195121951),\n",
       " ('RFC', 0.7121951219512195),\n",
       " ('MNBayes', 0.5853658536585366),\n",
       " ('BerBayes', 0.424390243902439)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"/Users/brianmccabe/Desktop/Early_Results_Selection_Biased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
