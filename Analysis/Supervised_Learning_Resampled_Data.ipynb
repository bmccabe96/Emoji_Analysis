{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "2  omg there s more on the ballot then just the p...          -0.7003   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...           0.5437   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         üò©  \n",
       "1            0.010101         üòä  \n",
       "2            0.000000         üò±  \n",
       "3            0.000000         üòä  \n",
       "4            0.021429         üòä  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2617.000000\n",
       "mean        0.596261\n",
       "std         0.307721\n",
       "min         0.000000\n",
       "25%         0.304228\n",
       "50%         0.690462\n",
       "75%         0.874311\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:00<00:00, 7810.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         üò©        0.000000  \n",
       "1            0.010101         üòä        0.111111  \n",
       "2            0.000000         üò±        0.000000  \n",
       "3            0.000000         üòä        0.000000  \n",
       "4            0.021429         üòä        0.037037  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:07<00:00, 331.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         üò©        0.000000   0.000000  \n",
       "1            0.010101         üòä        0.111111   0.010753  \n",
       "2            0.000000         üò±        0.000000   0.000000  \n",
       "3            0.000000         üòä        0.000000   0.000000  \n",
       "4            0.021429         üòä        0.037037   0.007576  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:00<00:00, 2755.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's gooooo to the zoooo. Wazzzzuppppp. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's goo to the zoo. Wazzupp. AABBCC\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1487\n",
       "üò©     625\n",
       "üò°     335\n",
       "üò±     170\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity', 'exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25066870462361485\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2457011845624761)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1487\n",
       "üò©     625\n",
       "üò°     335\n",
       "üò±     170\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üò°    255\n",
       "üòä    255\n",
       "üò©    255\n",
       "üò±    255\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "cry = df[df.top_emoji == 'üò©']\n",
    "happy = df[df.top_emoji == 'üòä']\n",
    "fear = df[df.top_emoji == 'üò±']\n",
    "anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "cry_downsampled = resample(cry,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number\n",
    "                          random_state=seed) \n",
    "\n",
    "happy_downsampled = resample(happy,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "fear_upsampled = resample(fear,\n",
    "                          replace=True, \n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "anger_downsampled = resample(anger,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "\n",
    "df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_downsampled])\n",
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity', 'exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'],\n",
    "                 'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "            \n",
    "#             ('embedding', Pipeline([\n",
    "#                 ('selector', ItemSelector(key='tweet')),\n",
    "#                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "#             ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization','profanity','sentiment_score','exclamation_points'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 1,#0.9,\n",
    "#             'embedding': 1,\n",
    "            'stats': 1 #1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x11b769a60>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (816, 796) - (204, 796)\n",
      "CPU times: user 366 ms, sys: 3.49 ms, total: 369 ms\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.4s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "lr_clf.fit(train_vec, y_train)\n",
    "test_preds = lr_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Testing Accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('LogReg')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('LogReg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.75      0.80      0.77        51\n",
      "           üò°       0.53      0.61      0.56        51\n",
      "           üò©       0.59      0.43      0.50        51\n",
      "           üò±       0.75      0.78      0.77        51\n",
      "\n",
      "    accuracy                           0.66       204\n",
      "   macro avg       0.66      0.66      0.65       204\n",
      "weighted avg       0.66      0.66      0.65       204\n",
      "\n",
      "----------------------------------------\n",
      "[[41  6  4  0]\n",
      " [ 7 31  7  6]\n",
      " [ 6 16 22  7]\n",
      " [ 1  6  4 40]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Linear SVC', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[.1,1,10],'gamma':[10,1,0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10], 'gamma': [10, 1, 0.1]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc=GridSearchCV(SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_svc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6422312556458898\n",
      "{'C': 10, 'gamma': 1}\n",
      "SVC(C=10, gamma=1)\n"
     ]
    }
   ],
   "source": [
    "print(grid_svc.best_score_)\n",
    "print(grid_svc.best_params_)\n",
    "print(grid_svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = grid_svc.best_estimator_.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid\n",
      "Testing Accuracy: 0.701\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('SVC Grid')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2457011845624761),\n",
       " ('LogReg', 0.6568627450980392),\n",
       " ('Linear SVC', 0.6568627450980392),\n",
       " ('SVC_Grid', 0.7009803921568627)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('SVC_Grid', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=250, random_state=seed,n_jobs=-1,verbose=1, class_weight='balanced')\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7059\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.84      0.92      0.88        51\n",
      "           üò°       0.60      0.59      0.59        51\n",
      "           üò©       0.58      0.49      0.53        51\n",
      "           üò±       0.76      0.82      0.79        51\n",
      "\n",
      "    accuracy                           0.71       204\n",
      "   macro avg       0.70      0.71      0.70       204\n",
      "weighted avg       0.70      0.71      0.70       204\n",
      "\n",
      "----------------------------------------\n",
      "[[47  3  1  0]\n",
      " [ 2 30 12  7]\n",
      " [ 6 14 25  6]\n",
      " [ 1  3  5 42]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.549\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.451\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2457011845624761),\n",
       " ('LogReg', 0.6568627450980392),\n",
       " ('Linear SVC', 0.6568627450980392),\n",
       " ('SVC_Grid', 0.7009803921568627),\n",
       " ('RFC', 0.7058823529411765),\n",
       " ('MNBayes', 0.5490196078431373),\n",
       " ('BerBayes', 0.45098039215686275)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.6176\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2457011845624761),\n",
       " ('LogReg', 0.6568627450980392),\n",
       " ('Linear SVC', 0.6568627450980392),\n",
       " ('SVC_Grid', 0.7009803921568627),\n",
       " ('RFC', 0.7058823529411765),\n",
       " ('MNBayes', 0.5490196078431373),\n",
       " ('BerBayes', 0.45098039215686275),\n",
       " ('PassiveAgg', 0.6176470588235294)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A BASELINE XGB\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(train_vec, y_train,eval_metric='merror')\n",
    "test_preds = xg.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Testing Accuracy: 0.6765\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('XGBoost')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('XGB', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2457011845624761),\n",
       " ('LogReg', 0.6568627450980392),\n",
       " ('Linear SVC', 0.6568627450980392),\n",
       " ('SVC_Grid', 0.7009803921568627),\n",
       " ('RFC', 0.7058823529411765),\n",
       " ('MNBayes', 0.5490196078431373),\n",
       " ('BerBayes', 0.45098039215686275),\n",
       " ('PassiveAgg', 0.6176470588235294),\n",
       " ('XGB', 0.6764705882352942)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.89      0.80      0.85        51\n",
      "           üò°       0.55      0.67      0.60        51\n",
      "           üò©       0.56      0.47      0.51        51\n",
      "           üò±       0.74      0.76      0.75        51\n",
      "\n",
      "    accuracy                           0.68       204\n",
      "   macro avg       0.68      0.68      0.68       204\n",
      "weighted avg       0.68      0.68      0.68       204\n",
      "\n",
      "---------------------------------------\n",
      "[[41  8  2  0]\n",
      " [ 2 34 10  5]\n",
      " [ 2 16 24  9]\n",
      " [ 1  4  7 39]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('---------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('smv_grid', grid_svc.best_estimator_), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "voting_clf.fit(train_vec, y_train)\n",
    "test_preds = voting_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "Testing Accuracy: 0.6863\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Voting')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Voting', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RFC', 0.7058823529411765),\n",
       " ('SVC_Grid', 0.7009803921568627),\n",
       " ('Voting', 0.6862745098039216),\n",
       " ('XGB', 0.6764705882352942),\n",
       " ('LogReg', 0.6568627450980392),\n",
       " ('Linear SVC', 0.6568627450980392),\n",
       " ('PassiveAgg', 0.6176470588235294),\n",
       " ('MNBayes', 0.5490196078431373),\n",
       " ('BerBayes', 0.45098039215686275),\n",
       " ('Dummy', 0.2457011845624761)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
