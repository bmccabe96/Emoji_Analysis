{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "2  omg there s more on the ballot then just the p...          -0.7003   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...           0.5437   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         üò©  \n",
       "1            0.010101         üòä  \n",
       "2            0.000000         üò±  \n",
       "3            0.000000         üòä  \n",
       "4            0.021429         üòä  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2771.000000\n",
       "mean        0.593511\n",
       "std         0.307254\n",
       "min         0.000000\n",
       "25%         0.301172\n",
       "50%         0.690462\n",
       "75%         0.871406\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:00<00:00, 7408.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         üò©        0.000000  \n",
       "1            0.010101         üòä        0.111111  \n",
       "2            0.000000         üò±        0.000000  \n",
       "3            0.000000         üòä        0.000000  \n",
       "4            0.021429         üòä        0.037037  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:06<00:00, 461.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         üò©        0.000000   0.000000  \n",
       "1            0.010101         üòä        0.111111   0.010753  \n",
       "2            0.000000         üò±        0.000000   0.000000  \n",
       "3            0.000000         üòä        0.000000   0.000000  \n",
       "4            0.021429         üòä        0.037037   0.007576  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:00<00:00, 5056.93it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's gooooo to the zoooo. Wazzzzuppppp. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's goo to the zoo. Wazzupp. AABBCC\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1562\n",
       "üò©     663\n",
       "üò°     363\n",
       "üò±     183\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity', 'exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2540599061710574\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.247924936845904)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1562\n",
       "üò©     663\n",
       "üò°     363\n",
       "üò±     183\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    274\n",
       "üò©    274\n",
       "üò°    274\n",
       "üò±    274\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "cry = df[df.top_emoji == 'üò©']\n",
    "happy = df[df.top_emoji == 'üòä']\n",
    "fear = df[df.top_emoji == 'üò±']\n",
    "anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "cry_downsampled = resample(cry,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number\n",
    "                          random_state=seed) \n",
    "\n",
    "happy_downsampled = resample(happy,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "fear_upsampled = resample(fear,\n",
    "                          replace=True, \n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "anger_downsampled = resample(anger,\n",
    "                          replace=False,\n",
    "                          n_samples=int(len(fear)*1.5), # match number \n",
    "                          random_state=seed) \n",
    "\n",
    "df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_downsampled])\n",
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity', 'exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'],\n",
    "                 'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "            \n",
    "#             ('embedding', Pipeline([\n",
    "#                 ('selector', ItemSelector(key='tweet')),\n",
    "#                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "#             ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization','profanity','sentiment_score','exclamation_points'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 1,#0.9,\n",
    "#             'embedding': 1,\n",
    "            'stats': 1 #1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x121ce9a60>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (876, 852) - (220, 852)\n",
      "CPU times: user 495 ms, sys: 3.84 ms, total: 499 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "lr_clf.fit(train_vec, y_train)\n",
    "test_preds = lr_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Testing Accuracy: 0.6727\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('LogReg')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('LogReg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.85      0.80      0.82        55\n",
      "           üò°       0.53      0.45      0.49        55\n",
      "           üò©       0.56      0.53      0.54        55\n",
      "           üò±       0.72      0.91      0.81        55\n",
      "\n",
      "    accuracy                           0.67       220\n",
      "   macro avg       0.67      0.67      0.67       220\n",
      "weighted avg       0.67      0.67      0.67       220\n",
      "\n",
      "----------------------------------------\n",
      "[[44  6  4  1]\n",
      " [ 7 25 16  7]\n",
      " [ 1 14 29 11]\n",
      " [ 0  2  3 50]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.6773\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Linear SVC', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[.1,1,10],'gamma':[10,1,0.1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10], 'gamma': [10, 1, 0.1]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc=GridSearchCV(SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_svc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030694879832811\n",
      "{'C': 1, 'gamma': 1}\n",
      "SVC(C=1, gamma=1)\n"
     ]
    }
   ],
   "source": [
    "print(grid_svc.best_score_)\n",
    "print(grid_svc.best_params_)\n",
    "print(grid_svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = grid_svc.best_estimator_.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid\n",
      "Testing Accuracy: 0.6909\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('SVC Grid')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.247924936845904),\n",
       " ('LogReg', 0.6727272727272727),\n",
       " ('Linear SVC', 0.6772727272727272),\n",
       " ('SVC_Grid', 0.6909090909090909)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('SVC_Grid', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=250, random_state=seed,n_jobs=-1,verbose=1, class_weight='balanced')\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7591\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.91      0.91      0.91        55\n",
      "           üò°       0.68      0.65      0.67        55\n",
      "           üò©       0.63      0.56      0.60        55\n",
      "           üò±       0.79      0.91      0.85        55\n",
      "\n",
      "    accuracy                           0.76       220\n",
      "   macro avg       0.75      0.76      0.75       220\n",
      "weighted avg       0.75      0.76      0.75       220\n",
      "\n",
      "----------------------------------------\n",
      "[[50  2  2  1]\n",
      " [ 4 36 12  3]\n",
      " [ 1 14 31  9]\n",
      " [ 0  1  4 50]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.5227\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.4545\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.247924936845904),\n",
       " ('LogReg', 0.6727272727272727),\n",
       " ('Linear SVC', 0.6772727272727272),\n",
       " ('SVC_Grid', 0.6909090909090909),\n",
       " ('RFC', 0.759090909090909),\n",
       " ('MNBayes', 0.5227272727272727),\n",
       " ('BerBayes', 0.45454545454545453)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.6591\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.247924936845904),\n",
       " ('LogReg', 0.6727272727272727),\n",
       " ('Linear SVC', 0.6772727272727272),\n",
       " ('SVC_Grid', 0.6909090909090909),\n",
       " ('RFC', 0.759090909090909),\n",
       " ('MNBayes', 0.5227272727272727),\n",
       " ('BerBayes', 0.45454545454545453),\n",
       " ('PassiveAgg', 0.6590909090909091)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A BASELINE XGB\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(train_vec, y_train,eval_metric='merror')\n",
    "test_preds = xg.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Testing Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('XGBoost')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('XGB', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.247924936845904),\n",
       " ('LogReg', 0.6727272727272727),\n",
       " ('Linear SVC', 0.6772727272727272),\n",
       " ('SVC_Grid', 0.6909090909090909),\n",
       " ('RFC', 0.759090909090909),\n",
       " ('MNBayes', 0.5227272727272727),\n",
       " ('BerBayes', 0.45454545454545453),\n",
       " ('PassiveAgg', 0.6590909090909091),\n",
       " ('XGB', 0.7)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.87      0.84      0.85        55\n",
      "           üò°       0.71      0.53      0.60        55\n",
      "           üò©       0.53      0.55      0.54        55\n",
      "           üò±       0.71      0.89      0.79        55\n",
      "\n",
      "    accuracy                           0.70       220\n",
      "   macro avg       0.70      0.70      0.70       220\n",
      "weighted avg       0.70      0.70      0.70       220\n",
      "\n",
      "---------------------------------------\n",
      "[[46  4  4  1]\n",
      " [ 4 29 17  5]\n",
      " [ 3  8 30 14]\n",
      " [ 0  0  6 49]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('---------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('smv_grid', grid_svc.best_estimator_), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "voting_clf.fit(train_vec, y_train)\n",
    "test_preds = voting_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "Testing Accuracy: 0.7455\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Voting')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Voting', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RFC', 0.759090909090909),\n",
       " ('Voting', 0.7454545454545455),\n",
       " ('XGB', 0.7),\n",
       " ('SVC_Grid', 0.6909090909090909),\n",
       " ('Linear SVC', 0.6772727272727272),\n",
       " ('LogReg', 0.6727272727272727),\n",
       " ('PassiveAgg', 0.6590909090909091),\n",
       " ('MNBayes', 0.5227272727272727),\n",
       " ('BerBayes', 0.45454545454545453),\n",
       " ('Dummy', 0.247924936845904)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
