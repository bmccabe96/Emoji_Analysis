{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('viridis')\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"”\",\"``\",\"“\",\"''\",\"’\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt_2(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  you need a President too I can be one for you ...           0.4033   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...           0.9690   \n",
       "3  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "4  omg there s more on the ballot then just the p...          -0.7003   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         😩  \n",
       "1            0.000000         😊  \n",
       "2            0.014286         😊  \n",
       "3            0.010101         😊  \n",
       "4            0.000000         😱  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4262.000000\n",
       "mean        0.597159\n",
       "std         0.307784\n",
       "min         0.000000\n",
       "25%         0.302875\n",
       "50%         0.700531\n",
       "75%         0.871406\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4262/4262 [00:00<00:00, 7191.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.701232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.984621</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  you need a President too I can be one for you ...         0.701232   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...         0.984621   \n",
       "3  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "4  omg there s more on the ballot then just the p...         0.148382   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         😩        0.000000  \n",
       "1            0.000000         😊        0.066667  \n",
       "2            0.014286         😊        0.000000  \n",
       "3            0.010101         😊        0.111111  \n",
       "4            0.000000         😱        0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4262/4262 [00:10<00:00, 416.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.701232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.984621</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  you need a President too I can be one for you ...         0.701232   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...         0.984621   \n",
       "3  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "4  omg there s more on the ballot then just the p...         0.148382   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         😩        0.000000   0.000000  \n",
       "1            0.000000         😊        0.066667   0.000000  \n",
       "2            0.014286         😊        0.000000   0.000000  \n",
       "3            0.010101         😊        0.111111   0.010753  \n",
       "4            0.000000         😱        0.000000   0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4262/4262 [00:00<00:00, 4411.59it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's!! gooooo to the zoooo!. Wazzzzuppppp!!!. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's!! goo to the zoo!. Wazzupp!!. AABBCC\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😊    2455\n",
       "😩     918\n",
       "😡     606\n",
       "😱     283\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a very clear and large class imbalance. In this notebook, all models try to handle this inbalance by balancing the weights given to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_ax = ['Happy', 'Sad', 'Angry', 'Fearful']\n",
    "y = df.top_emoji.value_counts()\n",
    "sns.barplot(x_ax, y)\n",
    "plt.title(\"Class Imbalance\")\n",
    "plt.ylabel(\"Count\")\n",
    "#plt.savefig(\"../pics/class_imbalance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24213984045049272\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😊    2455\n",
       "😩     918\n",
       "😡     606\n",
       "😱     283\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses class_weight = balanced, see other notebook for resampled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# cry = df[df.top_emoji == '😩']\n",
    "# happy = df[df.top_emoji == '😊']\n",
    "# fear = df[df.top_emoji == '😱']\n",
    "# anger = df[df.top_emoji == '😡']\n",
    "\n",
    "\n",
    "# cry_downsampled = resample(cry,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number\n",
    "#                           random_state=seed) \n",
    "\n",
    "# happy_downsampled = resample(happy,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# fear_upsampled = resample(fear,\n",
    "#                           replace=True, \n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# anger_upsampled = resample(anger,\n",
    "#                           replace=True,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "\n",
    "# df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "# df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        rs = []\n",
    "        for row in data.iterrows():\n",
    "            to_add = {}\n",
    "            for item in row[1:]:\n",
    "                for ind, val in zip(item.index, item.values):\n",
    "                    to_add[ind] = val\n",
    "            rs.append(to_add)\n",
    "        return rs\n",
    "#         return [{'cap':  row['capitalization'], 'prof': row['profanity'], \n",
    "#                  'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ItemSelector(['capitalization','sentiment_score','exclamation_points']).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = []\n",
    "for row in test.transform(X).iterrows():\n",
    "    to_add = {}\n",
    "    for item in row[1:]:\n",
    "        for ind, val in zip(item.index, item.values):\n",
    "            to_add[ind] = val\n",
    "    rs.append(to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'capitalization': 0.0,\n",
       "  'sentiment_score': 0.12613966536419197,\n",
       "  'exclamation_points': 0.0},\n",
       " {'capitalization': 0.06666666666666667,\n",
       "  'sentiment_score': 0.7012323414487527,\n",
       "  'exclamation_points': 0.0}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to help in the creation of our training arrays to include custom features as well as TF IDF and Word Embeddings.\n",
    "\n",
    "- NOTE: Word embeddings hurt the score, hence it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(input_selectors): \n",
    "    pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "\n",
    "                # Pipeline for pulling features from the text\n",
    "                ('text', Pipeline([\n",
    "                    ('selector', ItemSelector(key='tweet')),\n",
    "                    ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                        stop_words = None, preprocessor=clean_txt)),\n",
    "                ])),\n",
    "                # Text two does not remove stopwords and tries to find bi and trigrams\n",
    "                ('text_2', Pipeline([\n",
    "                    ('selector', ItemSelector(key='tweet')),\n",
    "                    ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(2, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                        stop_words = None, preprocessor=clean_txt_2)),\n",
    "                ])),\n",
    "\n",
    "    #             ('embedding', Pipeline([\n",
    "    #                 ('selector', ItemSelector(key='tweet')),\n",
    "    #                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "    #             ])),\n",
    "\n",
    "                # Pipeline for pulling metadata features\n",
    "                ('stats', Pipeline([\n",
    "                    ('selector', ItemSelector(key=input_selectors)),\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ])),\n",
    "\n",
    "            ],\n",
    "\n",
    "            # weight components in FeatureUnion\n",
    "            transformer_weights={\n",
    "                'text': 1,#0.9,\n",
    "                'text_2':1,\n",
    "    #             'embedding': 1,\n",
    "                'stats': 1 #1.5,\n",
    "            },\n",
    "        ))\n",
    "    ], verbose=True)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['capitalization','profanity','sentiment_score','exclamation_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cap = create_pipeline(['capitalization'])\n",
    "pipeline_prof = create_pipeline(['profanity'])\n",
    "pipeline_sent = create_pipeline(['sentiment_score'])\n",
    "pipeline_excl = create_pipeline(['exclamation_points'])\n",
    "pipeline_all = create_pipeline(['capitalization','profanity','sentiment_score','exclamation_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.1s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.1s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.1s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x123931f70>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('text_2',\n",
       "                                                 Pipeline(steps=[('sel...\n",
       "                                                                                  preprocessor=<function clean_txt_2 at 0x123923040>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1,\n",
       "                                                   'text_2': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cap.fit(X_train)\n",
    "pipeline_prof.fit(X_train)\n",
    "pipeline_sent.fit(X_train)\n",
    "pipeline_excl.fit(X_train)\n",
    "pipeline_all.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (3409, 7489) - (853, 7489)\n",
      "Checking that the shapes match: (3409, 7489) - (853, 7489)\n",
      "Checking that the shapes match: (3409, 7489) - (853, 7489)\n",
      "Checking that the shapes match: (3409, 7489) - (853, 7489)\n",
      "Checking that the shapes match: (3409, 7492) - (853, 7492)\n",
      "CPU times: user 18.2 s, sys: 208 ms, total: 18.4 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec_cap = pipeline_cap.transform(X_train)\n",
    "test_vec_cap = pipeline_cap.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_cap.shape, test_vec_cap.shape))\n",
    "\n",
    "train_vec_prof = pipeline_prof.transform(X_train)\n",
    "test_vec_prof = pipeline_prof.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_prof.shape, test_vec_prof.shape))\n",
    "\n",
    "train_vec_sent = pipeline_sent.transform(X_train)\n",
    "test_vec_sent = pipeline_sent.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_sent.shape, test_vec_sent.shape))\n",
    "\n",
    "train_vec_excl = pipeline_excl.transform(X_train)\n",
    "test_vec_excl = pipeline_excl.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_excl.shape, test_vec_excl.shape))\n",
    "\n",
    "train_vec_all = pipeline_all.transform(X_train)\n",
    "test_vec_all = pipeline_all.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_all.shape, test_vec_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = [train_vec_cap, train_vec_prof, train_vec_sent, train_vec_excl, train_vec_all]\n",
    "tests = [test_vec_cap, test_vec_prof, test_vec_sent, test_vec_excl, test_vec_all]\n",
    "labels = ['capitalization', 'profanity', 'sentiment_score', 'exclamation_points', 'all_custom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overarching Function for Iterative Modeling\n",
    "- Takes in a model\n",
    "- Runs that model against all the different train/test vecs with dif features\n",
    "- prints out accuracy and description of what train/test vec worked best\n",
    "- prints out confusion matrix and classification report for best version of the model\n",
    "- Returns best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, model_name, conditions=None, choices=None):\n",
    "    res = []\n",
    "    for train, test, label in zip(trains, tests, labels):\n",
    "        if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "            model.fit(train, y_train, eval_metric='mlogloss', sample_weight = np.select(conditions, choices, None))\n",
    "        else:\n",
    "            model.fit(train, y_train)\n",
    "        test_preds = model.predict(test)\n",
    "        accuracy = accuracy_score(y_test, test_preds)\n",
    "        res.append({'label': label, 'score': accuracy, 'test_preds': test_preds})\n",
    "    res = sorted(res, key = lambda x: x['score'], reverse=True)\n",
    "    print('RESULTS')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(f\"{model_name} with {res[0]['label']} features performed the best with an accuracy of {res[0]['score']}\")\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, res[0]['test_preds']))\n",
    "    print('----------------------------------------')\n",
    "    print(confusion_matrix(y_test, res[0]['test_preds']))\n",
    "    return res[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "log reg with all_custom features performed the best with an accuracy of 0.7397420867526378\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.89      0.92      0.91       491\n",
      "           😡       0.51      0.50      0.50       121\n",
      "           😩       0.57      0.54      0.56       184\n",
      "           😱       0.37      0.32      0.34        57\n",
      "\n",
      "    accuracy                           0.74       853\n",
      "   macro avg       0.58      0.57      0.58       853\n",
      "weighted avg       0.73      0.74      0.73       853\n",
      "\n",
      "----------------------------------------\n",
      "[[453  11  25   2]\n",
      " [ 24  60  27  10]\n",
      " [ 30  35 100  19]\n",
      " [  3  12  24  18]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "results.append(('log reg', get_model_results(lr_clf, 'log reg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633), ('log reg', 0.7397420867526378)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "linear svc with all_custom features performed the best with an accuracy of 0.7514654161781946\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.88      0.95      0.91       491\n",
      "           😡       0.54      0.48      0.51       121\n",
      "           😩       0.57      0.56      0.57       184\n",
      "           😱       0.41      0.28      0.33        57\n",
      "\n",
      "    accuracy                           0.75       853\n",
      "   macro avg       0.60      0.57      0.58       853\n",
      "weighted avg       0.73      0.75      0.74       853\n",
      "\n",
      "----------------------------------------\n",
      "[[464   6  20   1]\n",
      " [ 25  58  30   8]\n",
      " [ 34  33 103  14]\n",
      " [  3  11  27  16]]\n"
     ]
    }
   ],
   "source": [
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed) \n",
    "results.append(('linear svc', get_model_results(sv_clf, 'linear svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "svc with all_custom features performed the best with an accuracy of 0.7690504103165299\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.88      0.94      0.91       491\n",
      "           😡       0.63      0.42      0.50       121\n",
      "           😩       0.59      0.66      0.62       184\n",
      "           😱       0.57      0.35      0.43        57\n",
      "\n",
      "    accuracy                           0.77       853\n",
      "   macro avg       0.67      0.59      0.62       853\n",
      "weighted avg       0.76      0.77      0.76       853\n",
      "\n",
      "----------------------------------------\n",
      "[[463   5  22   1]\n",
      " [ 29  51  32   9]\n",
      " [ 35  22 122   5]\n",
      " [  2   3  32  20]]\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=15, class_weight='balanced', kernel='rbf', gamma='scale')\n",
    "results.append(('svc', get_model_results(svc, 'svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "rfc with all_custom features performed the best with an accuracy of 0.7596717467760844\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.85      0.95      0.90       491\n",
      "           😡       0.67      0.38      0.48       121\n",
      "           😩       0.56      0.70      0.62       184\n",
      "           😱       0.91      0.18      0.29        57\n",
      "\n",
      "    accuracy                           0.76       853\n",
      "   macro avg       0.75      0.55      0.57       853\n",
      "weighted avg       0.77      0.76      0.74       853\n",
      "\n",
      "----------------------------------------\n",
      "[[464   7  20   0]\n",
      " [ 31  46  43   1]\n",
      " [ 47   9 128   0]\n",
      " [  2   7  38  10]]\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=400, random_state=seed,n_jobs=-1,\n",
    "                                 class_weight='balanced',criterion='entropy' )\n",
    "results.append(('rfc', get_model_results(rfc_clf, 'rfc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('rfc', 0.7596717467760844)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "mn bayes with sentiment_score features performed the best with an accuracy of 0.6436107854630715\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.64      0.98      0.77       491\n",
      "           😡       0.80      0.20      0.32       121\n",
      "           😩       0.65      0.23      0.34       184\n",
      "           😱       1.00      0.05      0.10        57\n",
      "\n",
      "    accuracy                           0.64       853\n",
      "   macro avg       0.77      0.36      0.38       853\n",
      "weighted avg       0.69      0.64      0.57       853\n",
      "\n",
      "----------------------------------------\n",
      "[[480   1  10   0]\n",
      " [ 88  24   9   0]\n",
      " [141   1  42   0]\n",
      " [ 46   4   4   3]]\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "results.append(('mn bayes', get_model_results(mnb_clf, 'mn bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('rfc', 0.7596717467760844),\n",
       " ('mn bayes', 0.6436107854630715)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "bernoulli bayes with all_custom features performed the best with an accuracy of 0.5732708089097304\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.65      0.79      0.71       491\n",
      "           😡       0.75      0.10      0.18       121\n",
      "           😩       0.38      0.47      0.42       184\n",
      "           😱       0.38      0.05      0.09        57\n",
      "\n",
      "    accuracy                           0.57       853\n",
      "   macro avg       0.54      0.35      0.35       853\n",
      "weighted avg       0.58      0.57      0.53       853\n",
      "\n",
      "----------------------------------------\n",
      "[[388   3  95   5]\n",
      " [ 77  12  32   0]\n",
      " [ 97   1  86   0]\n",
      " [ 39   0  15   3]]\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "results.append(('bernoulli bayes', get_model_results(bb_clf, 'bernoulli bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('rfc', 0.7596717467760844),\n",
       " ('mn bayes', 0.6436107854630715),\n",
       " ('bernoulli bayes', 0.5732708089097304)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "pac with all_custom features performed the best with an accuracy of 0.7256740914419695\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.86      0.91      0.88       491\n",
      "           😡       0.50      0.50      0.50       121\n",
      "           😩       0.54      0.51      0.52       184\n",
      "           😱       0.47      0.28      0.35        57\n",
      "\n",
      "    accuracy                           0.73       853\n",
      "   macro avg       0.59      0.55      0.56       853\n",
      "weighted avg       0.71      0.73      0.72       853\n",
      "\n",
      "----------------------------------------\n",
      "[[449  13  28   1]\n",
      " [ 27  60  28   6]\n",
      " [ 43  36  94  11]\n",
      " [  5  11  25  16]]\n"
     ]
    }
   ],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "results.append(('pac', get_model_results(pac_clf, 'pac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('rfc', 0.7596717467760844),\n",
       " ('mn bayes', 0.6436107854630715),\n",
       " ('bernoulli bayes', 0.5732708089097304),\n",
       " ('pac', 0.7256740914419695)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.73574338,  4.64441417,  7.02886598, 15.0840708 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [\n",
    "   y_train=='😊', y_train=='😩',y_train== '😡',y_train=='😱'\n",
    "]\n",
    "\n",
    "choices = 1 / np.array(list(y_train.value_counts(normalize=True))) # Assign weights to each class\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "xgboost with all_custom features performed the best with an accuracy of 0.779601406799531\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.91      0.91      0.91       491\n",
      "           😡       0.54      0.47      0.50       121\n",
      "           😩       0.63      0.70      0.66       184\n",
      "           😱       0.61      0.60      0.60        57\n",
      "\n",
      "    accuracy                           0.78       853\n",
      "   macro avg       0.67      0.67      0.67       853\n",
      "weighted avg       0.78      0.78      0.78       853\n",
      "\n",
      "----------------------------------------\n",
      "[[446  16  26   3]\n",
      " [ 20  57  35   9]\n",
      " [ 22  24 128  10]\n",
      " [  1   8  14  34]]\n"
     ]
    }
   ],
   "source": [
    "xg = xgb.XGBClassifier(n_jobs=-1)\n",
    "results.append(('xgboost', get_model_results(xg, 'xgboost', conditions, choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2522290004692633),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('rfc', 0.7596717467760844),\n",
       " ('mn bayes', 0.6436107854630715),\n",
       " ('bernoulli bayes', 0.5732708089097304),\n",
       " ('pac', 0.7256740914419695),\n",
       " ('xgboost', 0.779601406799531)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "voting with all_custom features performed the best with an accuracy of 0.7631887456037515\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.84      0.96      0.90       491\n",
      "           😡       0.65      0.46      0.54       121\n",
      "           😩       0.59      0.58      0.59       184\n",
      "           😱       0.56      0.26      0.36        57\n",
      "\n",
      "    accuracy                           0.76       853\n",
      "   macro avg       0.66      0.57      0.60       853\n",
      "weighted avg       0.74      0.76      0.75       853\n",
      "\n",
      "----------------------------------------\n",
      "[[473   1  16   1]\n",
      " [ 32  56  27   6]\n",
      " [ 50  22 107   5]\n",
      " [  5   7  30  15]]\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('svc', svc), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #(\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "results.append(('voting', get_model_results(voting_clf, 'voting')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xgboost', 0.779601406799531),\n",
       " ('svc', 0.7690504103165299),\n",
       " ('voting', 0.7631887456037515),\n",
       " ('rfc', 0.7596717467760844),\n",
       " ('linear svc', 0.7514654161781946),\n",
       " ('log reg', 0.7397420867526378),\n",
       " ('pac', 0.7256740914419695),\n",
       " ('mn bayes', 0.6436107854630715),\n",
       " ('bernoulli bayes', 0.5732708089097304),\n",
       " ('Dummy', 0.2522290004692633)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Model Results\", fontsize=20)\n",
    "plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
