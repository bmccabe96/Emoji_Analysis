{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('viridis')\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"”\",\"``\",\"“\",\"''\",\"’\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt_2(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  you need a President too I can be one for you ...           0.4033   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...           0.9690   \n",
       "3  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "4  omg there s more on the ballot then just the p...          -0.7003   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         😩  \n",
       "1            0.000000         😊  \n",
       "2            0.014286         😊  \n",
       "3            0.010101         😊  \n",
       "4            0.000000         😱  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5019.000000\n",
       "mean        0.602604\n",
       "std         0.306544\n",
       "min         0.000000\n",
       "25%         0.307935\n",
       "50%         0.701232\n",
       "75%         0.872458\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5019/5019 [00:00<00:00, 7923.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.701232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.984621</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  you need a President too I can be one for you ...         0.701232   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...         0.984621   \n",
       "3  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "4  omg there s more on the ballot then just the p...         0.148382   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         😩        0.000000  \n",
       "1            0.000000         😊        0.066667  \n",
       "2            0.014286         😊        0.000000  \n",
       "3            0.010101         😊        0.111111  \n",
       "4            0.000000         😱        0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5019/5019 [00:15<00:00, 330.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😩</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you need a President too I can be one for you ...</td>\n",
       "      <td>0.701232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...</td>\n",
       "      <td>0.984621</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>😊</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>😱</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  you need a President too I can be one for you ...         0.701232   \n",
       "2  Dì cheer! Cheeeeeeeeeeeeer! salvinisciacallo C...         0.984621   \n",
       "3  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "4  omg there s more on the ballot then just the p...         0.148382   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         😩        0.000000   0.000000  \n",
       "1            0.000000         😊        0.066667   0.000000  \n",
       "2            0.014286         😊        0.000000   0.000000  \n",
       "3            0.010101         😊        0.111111   0.010753  \n",
       "4            0.000000         😱        0.000000   0.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5019/5019 [00:01<00:00, 3910.31it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's!! gooooo to the zoooo!. Wazzzzuppppp!!!. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's!! goo to the zoo!. Wazzupp!!. AABBCC\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😊    2922\n",
       "😩    1061\n",
       "😡     711\n",
       "😱     325\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a very clear and large class imbalance. In this notebook, all models try to handle this inbalance by balancing the weights given to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_ax = ['Happy', 'Sad', 'Angry', 'Fearful']\n",
    "y = df.top_emoji.value_counts()\n",
    "sns.barplot(x_ax, y)\n",
    "plt.title(\"Class Imbalance\")\n",
    "plt.ylabel(\"Count\")\n",
    "#plt.savefig(\"../pics/class_imbalance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24985056784219964\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "😊    2922\n",
       "😩    1061\n",
       "😡     711\n",
       "😱     325\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses class_weight = balanced, see other notebook for resampled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# cry = df[df.top_emoji == '😩']\n",
    "# happy = df[df.top_emoji == '😊']\n",
    "# fear = df[df.top_emoji == '😱']\n",
    "# anger = df[df.top_emoji == '😡']\n",
    "\n",
    "\n",
    "# cry_downsampled = resample(cry,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number\n",
    "#                           random_state=seed) \n",
    "\n",
    "# happy_downsampled = resample(happy,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# fear_upsampled = resample(fear,\n",
    "#                           replace=True, \n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# anger_upsampled = resample(anger,\n",
    "#                           replace=True,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "\n",
    "# df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "# df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        rs = []\n",
    "        for row in data.iterrows():\n",
    "            to_add = {}\n",
    "            for item in row[1:]:\n",
    "                for ind, val in zip(item.index, item.values):\n",
    "                    to_add[ind] = val\n",
    "            rs.append(to_add)\n",
    "        return rs\n",
    "#         return [{'cap':  row['capitalization'], 'prof': row['profanity'], \n",
    "#                  'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ItemSelector(['capitalization','sentiment_score','exclamation_points']).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = []\n",
    "for row in test.transform(X).iterrows():\n",
    "    to_add = {}\n",
    "    for item in row[1:]:\n",
    "        for ind, val in zip(item.index, item.values):\n",
    "            to_add[ind] = val\n",
    "    rs.append(to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'capitalization': 0.0,\n",
       "  'sentiment_score': 0.12613966536419197,\n",
       "  'exclamation_points': 0.0},\n",
       " {'capitalization': 0.06666666666666667,\n",
       "  'sentiment_score': 0.7012323414487527,\n",
       "  'exclamation_points': 0.0}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to help in the creation of our training arrays to include custom features as well as TF IDF and Word Embeddings.\n",
    "\n",
    "- NOTE: Word embeddings hurt the score, hence it is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(input_selectors): \n",
    "    pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "\n",
    "                # Pipeline for pulling features from the text\n",
    "                ('text', Pipeline([\n",
    "                    ('selector', ItemSelector(key='tweet')),\n",
    "                    ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                        stop_words = None, preprocessor=clean_txt)),\n",
    "                ])),\n",
    "                # Text two does not remove stopwords and tries to find bi and trigrams\n",
    "                ('text_2', Pipeline([\n",
    "                    ('selector', ItemSelector(key='tweet')),\n",
    "                    ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(2, 10), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                        stop_words = None, preprocessor=clean_txt_2)),\n",
    "                ])),\n",
    "\n",
    "    #             ('embedding', Pipeline([\n",
    "    #                 ('selector', ItemSelector(key='tweet')),\n",
    "    #                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "    #             ])),\n",
    "\n",
    "                # Pipeline for pulling metadata features\n",
    "                ('stats', Pipeline([\n",
    "                    ('selector', ItemSelector(key=input_selectors)),\n",
    "                    ('stats', TextStats()),  # returns a list of dicts\n",
    "                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "                ])),\n",
    "\n",
    "            ],\n",
    "\n",
    "            # weight components in FeatureUnion\n",
    "            transformer_weights={\n",
    "                'text': 1,#0.9,\n",
    "                'text_2':1,\n",
    "    #             'embedding': 1,\n",
    "                'stats': 1 #1.5,\n",
    "            },\n",
    "        ))\n",
    "    ], verbose=True)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['capitalization','profanity','sentiment_score','exclamation_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cap = create_pipeline(['capitalization'])\n",
    "pipeline_prof = create_pipeline(['profanity'])\n",
    "pipeline_sent = create_pipeline(['sentiment_score'])\n",
    "pipeline_excl = create_pipeline(['exclamation_points'])\n",
    "pipeline_all = create_pipeline(['capitalization','profanity','sentiment_score','exclamation_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   3.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   3.0s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.6s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.5s\n",
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   2.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x12602b040>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('text_2',\n",
       "                                                 Pipeline(steps=[('sel...\n",
       "                                                                                  preprocessor=<function clean_txt_2 at 0x12600b670>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1,\n",
       "                                                   'text_2': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_cap.fit(X_train)\n",
    "pipeline_prof.fit(X_train)\n",
    "pipeline_sent.fit(X_train)\n",
    "pipeline_excl.fit(X_train)\n",
    "pipeline_all.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (4015, 8972) - (1004, 8972)\n",
      "Checking that the shapes match: (4015, 8972) - (1004, 8972)\n",
      "Checking that the shapes match: (4015, 8972) - (1004, 8972)\n",
      "Checking that the shapes match: (4015, 8972) - (1004, 8972)\n",
      "Checking that the shapes match: (4015, 8975) - (1004, 8975)\n",
      "CPU times: user 32.7 s, sys: 360 ms, total: 33.1 s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec_cap = pipeline_cap.transform(X_train)\n",
    "test_vec_cap = pipeline_cap.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_cap.shape, test_vec_cap.shape))\n",
    "\n",
    "train_vec_prof = pipeline_prof.transform(X_train)\n",
    "test_vec_prof = pipeline_prof.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_prof.shape, test_vec_prof.shape))\n",
    "\n",
    "train_vec_sent = pipeline_sent.transform(X_train)\n",
    "test_vec_sent = pipeline_sent.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_sent.shape, test_vec_sent.shape))\n",
    "\n",
    "train_vec_excl = pipeline_excl.transform(X_train)\n",
    "test_vec_excl = pipeline_excl.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_excl.shape, test_vec_excl.shape))\n",
    "\n",
    "train_vec_all = pipeline_all.transform(X_train)\n",
    "test_vec_all = pipeline_all.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec_all.shape, test_vec_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = [train_vec_cap, train_vec_prof, train_vec_sent, train_vec_excl, train_vec_all]\n",
    "tests = [test_vec_cap, test_vec_prof, test_vec_sent, test_vec_excl, test_vec_all]\n",
    "labels = ['capitalization', 'profanity', 'sentiment_score', 'exclamation_points', 'all_custom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Overarching Function for Iterative Modeling\n",
    "- Takes in a model\n",
    "- Runs that model against all the different train/test vecs with dif features\n",
    "- prints out accuracy and description of what train/test vec worked best\n",
    "- prints out confusion matrix and classification report for best version of the model\n",
    "- Returns best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, model_name, conditions=None, choices=None):\n",
    "    res = []\n",
    "    for train, test, label in zip(trains, tests, labels):\n",
    "        if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "            model.fit(train, y_train, eval_metric='mlogloss', sample_weight = np.select(conditions, choices, None))\n",
    "        else:\n",
    "            model.fit(train, y_train)\n",
    "        test_preds = model.predict(test)\n",
    "        accuracy = accuracy_score(y_test, test_preds)\n",
    "        res.append({'label': label, 'score': accuracy, 'test_preds': test_preds})\n",
    "    res = sorted(res, key = lambda x: x['score'], reverse=True)\n",
    "    print('RESULTS')\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(f\"{model_name} with {res[0]['label']} features performed the best with an accuracy of {res[0]['score']}\")\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, res[0]['test_preds']))\n",
    "    print('----------------------------------------')\n",
    "    print(confusion_matrix(y_test, res[0]['test_preds']))\n",
    "    return res[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "log reg with sentiment_score features performed the best with an accuracy of 0.75\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.89      0.93      0.91       585\n",
      "           😡       0.49      0.45      0.47       142\n",
      "           😩       0.64      0.61      0.62       212\n",
      "           😱       0.30      0.28      0.29        65\n",
      "\n",
      "    accuracy                           0.75      1004\n",
      "   macro avg       0.58      0.57      0.57      1004\n",
      "weighted avg       0.74      0.75      0.74      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[542  24  14   5]\n",
      " [ 32  64  33  13]\n",
      " [ 33  26 129  24]\n",
      " [  4  16  27  18]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "results.append(('log reg', get_model_results(lr_clf, 'log reg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813), ('log reg', 0.75)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "linear svc with all_custom features performed the best with an accuracy of 0.7559760956175299\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.87      0.94      0.91       585\n",
      "           😡       0.52      0.44      0.48       142\n",
      "           😩       0.64      0.62      0.63       212\n",
      "           😱       0.31      0.22      0.25        65\n",
      "\n",
      "    accuracy                           0.76      1004\n",
      "   macro avg       0.58      0.55      0.57      1004\n",
      "weighted avg       0.74      0.76      0.74      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[551  17  14   3]\n",
      " [ 37  63  35   7]\n",
      " [ 37  23 131  21]\n",
      " [  6  19  26  14]]\n"
     ]
    }
   ],
   "source": [
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed) \n",
    "results.append(('linear svc', get_model_results(sv_clf, 'linear svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "svc with all_custom features performed the best with an accuracy of 0.7828685258964143\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.88      0.96      0.92       585\n",
      "           😡       0.64      0.38      0.48       142\n",
      "           😩       0.62      0.75      0.68       212\n",
      "           😱       0.52      0.22      0.30        65\n",
      "\n",
      "    accuracy                           0.78      1004\n",
      "   macro avg       0.66      0.57      0.59      1004\n",
      "weighted avg       0.77      0.78      0.76      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[560   9  16   0]\n",
      " [ 36  54  45   7]\n",
      " [ 34  14 158   6]\n",
      " [  7   8  36  14]]\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=15, class_weight='balanced', kernel='rbf', gamma='scale')\n",
    "results.append(('svc', get_model_results(svc, 'svc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "rfc with all_custom features performed the best with an accuracy of 0.7739043824701195\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.85      0.95      0.90       585\n",
      "           😡       0.60      0.45      0.51       142\n",
      "           😩       0.63      0.70      0.66       212\n",
      "           😱       0.88      0.11      0.19        65\n",
      "\n",
      "    accuracy                           0.77      1004\n",
      "   macro avg       0.74      0.55      0.57      1004\n",
      "weighted avg       0.77      0.77      0.75      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[558  10  17   0]\n",
      " [ 42  64  36   0]\n",
      " [ 45  18 148   1]\n",
      " [ 10  15  33   7]]\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=400, random_state=seed,n_jobs=-1,\n",
    "                                 class_weight='balanced',criterion='entropy' )\n",
    "results.append(('rfc', get_model_results(rfc_clf, 'rfc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "mn bayes with sentiment_score features performed the best with an accuracy of 0.6543824701195219\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.65      0.98      0.78       585\n",
      "           😡       0.81      0.20      0.33       142\n",
      "           😩       0.63      0.23      0.33       212\n",
      "           😱       1.00      0.06      0.12        65\n",
      "\n",
      "    accuracy                           0.65      1004\n",
      "   macro avg       0.77      0.37      0.39      1004\n",
      "weighted avg       0.69      0.65      0.58      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[576   1   8   0]\n",
      " [104  29   9   0]\n",
      " [161   3  48   0]\n",
      " [ 47   3  11   4]]\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "results.append(('mn bayes', get_model_results(mnb_clf, 'mn bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195),\n",
       " ('mn bayes', 0.6543824701195219)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "bernoulli bayes with exclamation_points features performed the best with an accuracy of 0.6225099601593626\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.66      0.88      0.75       585\n",
      "           😡       0.69      0.17      0.27       142\n",
      "           😩       0.46      0.39      0.42       212\n",
      "           😱       0.50      0.06      0.11        65\n",
      "\n",
      "    accuracy                           0.62      1004\n",
      "   macro avg       0.58      0.37      0.39      1004\n",
      "weighted avg       0.61      0.62      0.57      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[515  10  56   4]\n",
      " [ 93  24  25   0]\n",
      " [130   0  82   0]\n",
      " [ 44   1  16   4]]\n"
     ]
    }
   ],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "results.append(('bernoulli bayes', get_model_results(bb_clf, 'bernoulli bayes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195),\n",
       " ('mn bayes', 0.6543824701195219),\n",
       " ('bernoulli bayes', 0.6225099601593626)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "pac with all_custom features performed the best with an accuracy of 0.7420318725099602\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.85      0.94      0.89       585\n",
      "           😡       0.49      0.46      0.47       142\n",
      "           😩       0.64      0.55      0.59       212\n",
      "           😱       0.33      0.20      0.25        65\n",
      "\n",
      "    accuracy                           0.74      1004\n",
      "   macro avg       0.58      0.54      0.55      1004\n",
      "weighted avg       0.72      0.74      0.73      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[550  18  14   3]\n",
      " [ 43  65  28   6]\n",
      " [ 48  29 117  18]\n",
      " [  7  22  23  13]]\n"
     ]
    }
   ],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "results.append(('pac', get_model_results(pac_clf, 'pac')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195),\n",
       " ('mn bayes', 0.6543824701195219),\n",
       " ('bernoulli bayes', 0.6225099601593626),\n",
       " ('pac', 0.7420318725099602)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.71801455,  4.72909305,  7.05623902, 15.44230769])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions = [\n",
    "   y_train=='😊', y_train=='😩',y_train== '😡',y_train=='😱'\n",
    "]\n",
    "\n",
    "choices = 1 / np.array(list(y_train.value_counts(normalize=True))) # Assign weights to each class\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "xgboost with all_custom features performed the best with an accuracy of 0.7848605577689243\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.92      0.92      0.92       585\n",
      "           😡       0.56      0.51      0.53       142\n",
      "           😩       0.65      0.71      0.68       212\n",
      "           😱       0.48      0.45      0.46        65\n",
      "\n",
      "    accuracy                           0.78      1004\n",
      "   macro avg       0.65      0.65      0.65      1004\n",
      "weighted avg       0.78      0.78      0.78      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[536  25  22   2]\n",
      " [ 21  72  37  12]\n",
      " [ 19  24 151  18]\n",
      " [  5   7  24  29]]\n"
     ]
    }
   ],
   "source": [
    "xg = xgb.XGBClassifier(n_jobs=-1)\n",
    "results.append(('xgboost', get_model_results(xg, 'xgboost', conditions, choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24227933851364813),\n",
       " ('log reg', 0.75),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195),\n",
       " ('mn bayes', 0.6543824701195219),\n",
       " ('bernoulli bayes', 0.6225099601593626),\n",
       " ('pac', 0.7420318725099602),\n",
       " ('xgboost', 0.7848605577689243)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS\n",
      "-------------------------------------------------------------------------------------------------\n",
      "voting with all_custom features performed the best with an accuracy of 0.7689243027888446\n",
      "-------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           😊       0.84      0.97      0.90       585\n",
      "           😡       0.61      0.42      0.49       142\n",
      "           😩       0.66      0.64      0.65       212\n",
      "           😱       0.48      0.18      0.27        65\n",
      "\n",
      "    accuracy                           0.77      1004\n",
      "   macro avg       0.65      0.55      0.58      1004\n",
      "weighted avg       0.74      0.77      0.75      1004\n",
      "\n",
      "----------------------------------------\n",
      "[[565   9  10   1]\n",
      " [ 47  59  33   3]\n",
      " [ 52  15 136   9]\n",
      " [ 11  14  28  12]]\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('svc', svc), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #(\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "results.append(('voting', get_model_results(voting_clf, 'voting')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xgboost', 0.7848605577689243),\n",
       " ('svc', 0.7828685258964143),\n",
       " ('rfc', 0.7739043824701195),\n",
       " ('voting', 0.7689243027888446),\n",
       " ('linear svc', 0.7559760956175299),\n",
       " ('log reg', 0.75),\n",
       " ('pac', 0.7420318725099602),\n",
       " ('mn bayes', 0.6543824701195219),\n",
       " ('bernoulli bayes', 0.6225099601593626),\n",
       " ('Dummy', 0.24227933851364813)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAF6CAYAAADf+gS3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df7gcdX33/+fLBEQUoUhEyg+hFqVohWpAUaxYpIKKkYI34A8Eq7lR0WKrt/R7q6XqVWut1ksBI1hAWwQVRANG0aKA8kMTBOSHYHMjSopKQEV+CoH394+ZQ5aTc052kjM5J8nzcV3nOjOzn5197+zs7Gs/+9nZVBWSJEmShveoqS5AkiRJWtsYoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSWtIkr2SVJJjV3M9h7frOXxyKpv+2vt7wVTXIUkjDNGS1llt8KokDyV5ygTtvjPQ9vA1WOIaMRC6B/9+n+RnSU5LsstU17gqkhzb3pe9proWSeufmVNdgCT1bBnNse6vgf9v9IVJdgReONBuXXYV8JV2+vHA84FXAwcm2buqLp6yyiRpLWNPtKR13a+ARcARScYKyW8EApy7RquaGldW1bHt399W1XOATwOPBj44xbVJ0lrFEC1pfXAS8CTg5YMLk2wAvB64BLh2vCsn2THJ55L8T5L7k9zSzu84Tvstk/x7kl8luTfJlUleP1GBSTZP8qEkP26vc0eS85P8Zed7282/t/93G6OmmUnekuSyJL9Lck+SK5IclWSF148kr2hr/kU7XOSWJBcmecuodjcluWmsYoYdotFe/x/a2cHhODXQZssk/5rkhiR3J/ltO31qkj+aaP2StDLr+keXkgRwOvAxml7nrwwsfwWwJXAM8MdjXTHJbsB/AZsA84HrgJ2A1wBz2mEQiwbaP4EmlP8R8L32bytgHvDNcW7jycAFwPbAd4FvAI+lCf3fSPK/q+qk7nd7KGn/PzCqpg2Ac4CXADcAnwfuA14EfBJ4DvC6gfZzaXq1f9le7zbgicAzgSOAEya57o8Dr6QZivNZ4KZR9W8MXAw8BfhWW1OAJwNzgDOBGye5JknrEUO0pHVeVd2Z5Azg8CTbVNWS9qI3Ab8DvsjY46UDfI5m/PBrq+q0gcsOBs4A/jPJzlX1UHvRh2gC9Mer6h0D7Y8DLh2nxM/ShLtDq+qMgetsRhOuP5FkflX9qvu9X6k3tf+/N2r5/6UJ0McBR1fVg21NM4ATgTckObOqvtq2/9/A/cAuVXXr4IqSbDHZRVfVx9vt80Lg1Kq6YFSTvWkC9CMeh7aeDWmGsEjSKnM4h6T1xUnADOAN8HDv7z7AaVV1zzjXeR5Nr/OlgwEaoKq+QBM8nwbs2a5zA5oe6juBY0e1XwQ8Yh3tdXahCYJnDQbo9jq/pRmysBFw4PB3dVy7tsMljk3ysSQLaXrnbwH+bqCmRwFH0fQqv2MkQLc1Pdi2LZr7OmgZo3q02+vcNgm1r6p7Ry+oqvur6s6pKEbSusOeaEnrhar6fpKraXpQP0gTHh9FE67H86z2/7fHufzbNAH6z4CLaAL3xsB3q+qOMdpfQDMGe9Ae7f9Nxzl/9Kz2/59MUOewdmn/Bv0ceEFV/Xxg2VOBJwD/Dbyn6ZBfwb2jajoN+ChwbZIvABcCF1fV0kmoe1VcCPwPcEySZwELaIZ3XDn4pkCSVpUhWtL65CTgE8C+NON0L6+qKyZov2n7/xfjXD6yfLNR7ccbdvHLMZY9of2/T/s3nsdNcNmwPltVh7fDVJ5Ic9q/DwLnJNljoEd+pKYdWf7lvQlrqqqPJbkNeAvwduBooJJcCLxrcNz4mlBVv0vyXOAfaca+v6S96LYkJwAfrKoVes0laVgO55C0PvkPmh7UTwNb04ztnchIb/KTxrl8q1HtRv5vOU77sdYzcp2/qapM8HfESmodWjV+VVX/RNN7/EweeYq7kZrOXklNO4xa7+eq6rk0IfxlNGf++HPgvCRPHGj6EON34mw2zvLOqmpJVf01zRuGZ9CE+9uB97V/krTKDNGS1hvtGOMzgW2Au2nO2jGRkV7qvca5fGT5D9v/1wP30Iw93nSC9oMua/+/YCW19OX9wFLgqCQjofh64LfAc9tx3p1U1W+rakFVvQk4FdicR96/3wBbjrPu2R1uamRYxoyV1FNVdW1VfZLlvf2v7HA7krQCQ7Sk9c17gAOAlwzx5bKLaU7vtmeSgwYvaOf/HPgJ7Zkt2uEBp9GcDu/YUe1ns+IX8Ua+cPhd4K+SvGGsIpL86aie3EnTboMPAxvQ1lxVy2hOY7cVzZlBHjNGTVsl2Xlgft9xfsxmpO7BL2/+gKYn+hG962l+cv35Hcq/vf2/3Rj1PSPJ9mNcZ+RTgvG+TCpJQ3FMtKT1SvsFup+vtGHTttofSfkW8IUkX6XppX0aTU/mncBhA6e3g+ZUeXsDR7fBeeQ80QfTfLntFWPc1KtpvqT470neDnyfpid4G5qhFs+g+QLirWNcdzKcQHPGjdcm+XBVXQd8gOZLiEcC+yf5Ns0X9Z5IM1b6+TSnwbuuXccZwH1JvkdzzubQ9D7vBlxOc67tEZ+kCdCfSrI3cHN7W8+j+eXIR/wozgS+QzM05ENJnkHTw01VfRB4MfCxJJfQPGa30mzPOe11PjLkbUjSmOyJlqQJVNX3aYLg52mC7Ltowt7pwG7t5YPtb6MJmKfQnK3jaGBX4M3Av41zG0uAZ9OE0gdpeqzf3t7Oz2nOwXz1JN+1wdu/l+b81o+iCc8jveqvBA6j6Y1/OU3Q3rdt914eecq+Y2jOg/0smi8XHkHTu/1u4EWDX+JrQ/qLaXr69wfm0pxjeg+awD1s3T+mOdvJL9vb/MBI/cB5ND/IshFNcP47mk8OvkVzNpIzh70dSRpLqmrlrSRJkiQ9zJ5oSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjpa684TvcUWW9T2228/1WVIkiRpHXf55ZffVlWzxrpsrQvR22+/PYsWLZrqMiRJkrSOS/Kz8S5zOIckSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI66jVEJ9k3yQ1JFic5ZozLN01yTpKrklyb5Ig+65EkSZImQ28hOskM4HhgP2Bn4NAkO49q9lbguqraBdgL+GiSDfuqSZIkSZoMffZE7w4srqobq+p+4Axgzqg2BWySJMDjgF8Dy3qsSZIkSVptfYborYGbB+aXtMsGHQf8CXALcDXwN1X10OgVJZmbZFGSRUuXLu2rXkmSJGkofYbojLGsRs2/BLgS+ENgV+C4JI9f4UpVJ1bV7KqaPWvWmD8aI0mSJK0xfYboJcC2A/Pb0PQ4DzoC+HI1FgM/BXbqsSZJkiRptfUZohcCOybZof2y4CHA/FFtfg7sDZBkS+BpwI091iRJkiSttpl9rbiqliU5CjgPmAGcXFXXJjmyvXwe8AHg1CRX0wz/eHdV3dZXTZIkSdJk6C1EA1TVAmDBqGXzBqZvAf6yzxokSZKkydZriF7Tln7qP6e6hEk3682vneoSJEmSNMo6FaK13PXHjz4l99pvp7d+dapLkCRJAnr+2W9JkiRpXWSIliRJkjoyREuSJEkdOSZa67wzT9l3qkuYdAcd8Y2pLkGSpPWaIVpajxz7xZdMdQmT7tj/dd5UlyBJWg85nEOSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjjw7h6T10kvP/uBUlzDpFhzwnqkuQZLWG/ZES5IkSR3ZEy1J67mXn3naVJcw6c496DVTXYKkdZw90ZIkSVJHhmhJkiSpI4dzSJLUOuCs7011CZPu7AP3nOoSpHWSPdGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkeGaEmSJKkjQ7QkSZLUkSFakiRJ6qjXEJ1k3yQ3JFmc5JgxLn9Xkivbv2uSPJhk8z5rkiRJklZXbyE6yQzgeGA/YGfg0CQ7D7apqo9U1a5VtSvw98CFVfXrvmqSJEmSJkOfPdG7A4ur6saquh84A5gzQftDgdN7rEeSJEmaFH2G6K2Bmwfml7TLVpBkY2Bf4Kwe65EkSZImRZ8hOmMsq3Ha7g9cPN5QjiRzkyxKsmjp0qWTVqAkSZK0KvoM0UuAbQfmtwFuGaftIUwwlKOqTqyq2VU1e9asWZNYoiRJktRdnyF6IbBjkh2SbEgTlOePbpRkU+CFwFd7rEWSJEmaNDP7WnFVLUtyFHAeMAM4uaquTXJke/m8tukBwDer6u6+apEkSZImU28hGqCqFgALRi2bN2r+VODUPuuQJEmSJpO/WChJkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkczp7oASZI0/Rx/9q+muoRJ99YDtpzqErQOsSdakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOpo51QVIkiRNZ1d85tapLmHS/dkbnzjVJaz17ImWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqaNeQ3SSfZPckGRxkmPGabNXkiuTXJvkwj7rkSRJkiZDb6e4SzIDOB7YB1gCLEwyv6quG2izGXACsG9V/TyJ51uRJEnStNdnT/TuwOKqurGq7gfOAOaMavNq4MtV9XOAqlr3TsQoSZKkdU6fIXpr4OaB+SXtskFPBf4gyQVJLk9y2FgrSjI3yaIki5YuXdpTuZIkSdJw+gzRGWNZjZqfCTwbeBnwEuC9SZ66wpWqTqyq2VU1e9asWZNfqSRJktRBnz/7vQTYdmB+G+CWMdrcVlV3A3cnuQjYBfhJj3VJkiRJq6XPnuiFwI5JdkiyIXAIMH9Um68CL0gyM8nGwHOAH/dYkyRJkrTaeuuJrqplSY4CzgNmACdX1bVJjmwvn1dVP07yDeBHwEPAZ6rqmr5qkiRJkiZDn8M5qKoFwIJRy+aNmv8I8JE+65AkSZImk79YKEmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkeGaEmSJKkjQ7QkSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpo15DdJJ9k9yQZHGSY8a4fK8kdyS5sv17X5/1SJIkSZNhZl8rTjIDOB7YB1gCLEwyv6quG9X0u1X18r7qkCRJkiZbnz3RuwOLq+rGqrofOAOY0+PtSZIkSWtEnyF6a+Dmgfkl7bLR9khyVZKvJ3l6j/VIkiRJk6K34RxAxlhWo+Z/CDy5qu5K8lLgK8COK6womQvMBdhuu+0mu05JkiSpkz57opcA2w7MbwPcMtigqn5XVXe10wuADZJsMXpFVXViVc2uqtmzZs3qsWRJkiRp5foM0QuBHZPskGRD4BBg/mCDJE9KknZ697ae23usSZIkSVptvQ3nqKplSY4CzgNmACdX1bVJjmwvnwccBLw5yTLgXuCQqho95EOSJEmaVvocEz0yRGPBqGXzBqaPA47rswZJkiRpsvmLhZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSRysN0UlensSwLUmSJLWGCceHAP+d5F+S/EnfBUmSJEnT3UpDdFW9Fvgz4P8BpyS5NMncJJv0Xp0kSZI0DQ01TKOqfgecBZwBbAUcAPwwydt6rE2SJEmaloYZE71/krOBbwMbALtX1X7ALsA7e65PkiRJmnZmDtHmVcC/VdVFgwur6p4kb+inLEmSJGn6GiZE/wPwi5GZJI8Btqyqm6rq/N4qkyRJkqapYcZEfwl4aGD+wXaZJEmStF4aJkTPrKr7R2ba6Q37K0mSJEma3oYJ0UuTvGJkJskc4Lb+SpIkSZKmt2HGRB8JnJbkOCDAzcBhvVYlSZIkTWMrDdFV9f+A5yZ5HJCqurP/siRJkqTpa5ieaJK8DHg6sFESAKrq/T3WJUmSJE1bw/zYyjzgYOBtNMM5XgU8uee6JEmSpGlrmC8WPq+qDgN+U1X/COwBbNtvWZIkSdL0NUyIvq/9f0+SPwQeAHboryRJkiRpehtmTPQ5STYDPgL8ECjgpF6rkiRJkqaxCUN0kkcB51fVb4GzkpwLbFRVd6yR6iRJkqRpaMLhHFX1EPDRgfnfG6AlSZK0vhtmTPQ3kxyYkXPbSZIkSeu5YcZE/y3wWGBZkvtoTnNXVfX4XiuTJEmSpqlhfrFwkzVRiCRJkrS2GObHVv58rL9hVp5k3yQ3JFmc5JgJ2u2W5MEkB3UpXpIkSZoKwwzneNfA9EbA7sDlwF9MdKUkM4DjgX2AJcDCJPOr6rox2n0YOK9D3ZIkSdKUGWY4x/6D80m2Bf5liHXvDiyuqhvb650BzAGuG9XubcBZwG7DFCxJkiRNtWHOzjHaEuAZQ7TbGrh51PW2HmyQZGvgAGDeRCtKMjfJoiSLli5d2rFcSZIkaXKttCc6ySdpfqUQmtC9K3DVEOse65R4NWr+48C7q+rBic6gV1UnAicCzJ49e/Q6JEmSpDVqmDHRiwamlwGnV9XFQ1xvCbDtwPw2wC2j2swGzmgD9BbAS5Msq6qvDLF+SZIkaUoME6LPBO6rqgeh+SJgko2r6p6VXG8hsGOSHYD/AQ4BXj3YoKp2GJlOcipwrgFakiRJ090wY6LPBx4zMP8Y4L9WdqWqWgYcRXPWjR8DX6yqa5McmeTIVSlWkiRJmg6G6YneqKruGpmpqruSbDzMyqtqAbBg1LIxv0RYVYcPs05JkiRpqg3TE313kmeNzCR5NnBvfyVJkiRJ09swPdFHA19KMvKlwK2Ag/srSZIkSZrehvmxlYVJdgKeRnPauuur6oHeK5MkSZKmqZUO50jyVuCxVXVNVV0NPC7JW/ovTZIkSZqehhkT/aaq+u3ITFX9BnhTfyVJkiRJ09swIfpRGfg5wSQzgA37K0mSJEma3ob5YuF5wBeTzKP52e4jga/3WpUkSZI0jQ0Tot8NzAXeTPPFwitoztAhSZIkrZdWOpyjqh4CLgNuBGYDe9P8AqEkSZK0Xhq3JzrJU4FDgEOB24EvAFTVi9ZMaZIkSdL0NNFwjuuB7wL7V9VigCTvWCNVSZIkSdPYRMM5DgR+CXwnyUlJ9qYZEy1JkiSt18YN0VV1dlUdDOwEXAC8A9gyyaeS/OUaqk+SJEmadob5YuHdVXVaVb0c2Aa4Ejim98okSZKkaWqYH1t5WFX9uqo+XVV/0VdBkiRJ0nTXKURLkiRJMkRLkiRJnRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkddRriE6yb5IbkixOcswYl89J8qMkVyZZlGTPPuuRJEmSJsPMvlacZAZwPLAPsARYmGR+VV030Ox8YH5VVZJnAl8EduqrJkmSJGky9NkTvTuwuKpurKr7gTOAOYMNququqqp29rFAIUmSJE1zfYborYGbB+aXtMseIckBSa4Hvga8YawVJZnbDvdYtHTp0l6KlSRJkobVZ4jOGMtW6GmuqrOraifglcAHxlpRVZ1YVbOravasWbMmuUxJkiSpmz5D9BJg24H5bYBbxmtcVRcBT0myRY81SZIkSautzxC9ENgxyQ5JNgQOAeYPNkjyx0nSTj8L2BC4vceaJEmSpNXW29k5qmpZkqOA84AZwMlVdW2SI9vL5wEHAocleQC4Fzh44IuGkiRJ0rTUW4gGqKoFwIJRy+YNTH8Y+HCfNUiSJEmTzV8slCRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkeGaEmSJKkjQ7QkSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOpo51QVIkiRp7fCrj18+1SVMui2PfvYqXc+eaEmSJKkjQ7QkSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR11GuITrJvkhuSLE5yzBiXvybJj9q/S5Ls0mc9kiRJ0mToLUQnmQEcD+wH7AwcmmTnUc1+Crywqp4JfAA4sa96JEmSpMnSZ0/07sDiqrqxqu4HzgDmDDaoqkuq6jft7GXANj3WI0mSJE2KPkP01sDNA/NL2mXj+Wvg6z3WI0mSJE2KmT2uO2MsqzEbJi+iCdF7jnP5XGAuwHbbbTdZ9UmSJEmrpM+e6CXAtgPz2wC3jG6U5JnAZ4A5VXX7WCuqqhOranZVzZ41a1YvxUqSJEnD6jNELwR2TLJDkg2BQ4D5gw2SbAd8GXhdVf2kx1okSZKkSdPbcI6qWpbkKOA8YAZwclVdm+TI9vJ5wPuAJwAnJAFYVlWz+6pJkiRJmgx9jommqhYAC0Ytmzcw/UbgjX3WIEmSJE02f7FQkiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkeGaEmSJKkjQ7QkSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpo15DdJJ9k9yQZHGSY8a4fKcklyb5fZJ39lmLJEmSNFlm9rXiJDOA44F9gCXAwiTzq+q6gWa/Bt4OvLKvOiRJkqTJ1mdP9O7A4qq6saruB84A5gw2qKpbq2oh8ECPdUiSJEmTqs8QvTVw88D8knZZZ0nmJlmUZNHSpUsnpThJkiRpVfUZojPGslqVFVXViVU1u6pmz5o1azXLkiRJklZPnyF6CbDtwPw2wC093p4kSZK0RvQZohcCOybZIcmGwCHA/B5vT5IkSVojejs7R1UtS3IUcB4wAzi5qq5NcmR7+bwkTwIWAY8HHkpyNLBzVf2ur7okSZKk1dVbiAaoqgXAglHL5g1M/5JmmIckSZK01vAXCyVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkdWSIliRJkjoyREuSJEkdGaIlSZKkjgzRkiRJUkeGaEmSJKkjQ7QkSZLUkSFakiRJ6sgQLUmSJHVkiJYkSZI6MkRLkiRJHRmiJUmSpI4M0ZIkSVJHhmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOjJES5IkSR0ZoiVJkqSODNGSJElSR4ZoSZIkqSNDtCRJktSRIVqSJEnqyBAtSZIkddRriE6yb5IbkixOcswYlyfJJ9rLf5TkWX3WI0mSJE2G3kJ0khnA8cB+wM7AoUl2HtVsP2DH9m8u8Km+6pEkSZImS5890bsDi6vqxqq6HzgDmDOqzRzgc9W4DNgsyVY91iRJkiSttj5D9NbAzQPzS9plXdtIkiRJ00qqqp8VJ68CXlJVb2znXwfsXlVvG2jzNeBDVfW9dv584P9U1eWj1jWXZrgHwNOAG3opupstgNumuohpwm2xnNtiObfFcm6LhtthObfFcm6L5dwWy02XbfHkqpo11gUze7zRJcC2A/PbALesQhuq6kTgxMkucHUkWVRVs6e6junAbbGc22I5t8VybouG22E5t8Vybovl3BbLrQ3bos/hHAuBHZPskGRD4BBg/qg284HD2rN0PBe4o6p+0WNNkiRJ0mrrrSe6qpYlOQo4D5gBnFxV1yY5sr18HrAAeCmwGLgHOKKveiRJkqTJ0udwDqpqAU1QHlw2b2C6gLf2WUOPptXwkinmtljObbGc22I5t0XD7bCc22I5t8Vybovlpv226O2LhZIkSdK6yp/9liRJkjoyRI8jyU1Jtuhp3bsmeWkf69bUSPKqJD9O8p2prmUqJNk+yasH5mcn+cRU1tRFkrva/3+Y5Myprme6GtlO0uqYDvtRe8y6ZqrrGDGYOVZ2PEqyV5Jz13SNw0ryYJIrk1yb5Kokf5tkncyb6+SdWgvsSvOFSq0DkgR4E/CWqnrRVNczRbYHHg7RVbWoqt4+deWsmqq6paoO6vM2kvT6XZS1jdtDa6M1sd+uieNRT+6tql2r6unAPjR55x+muKZerDchOsluSX6UZKMkj23fIT0zyQnt9LlJFiQZ3GHfleQH7d8ft+t5cpLz23Wdn2S7lSx/VZJr2ndjF7Wn+3s/cHD7Tu3gNb4xOmq319fa+3BNktcn+eLA5XslOaed3jfJD9u2509d1f1qezF+nOQE4CGaA8W8JB9JMiPJvya5ut0f3raS1U1LST6c5C0D88cm+bv2Pl7T3r+R/fefgRe0+/Q7BntK2uudnOSCJDcmefvAOt+b5Pok30pyepJ3rtl7+UiDvVNJDk/y5STfSPLfSf5loN1fJrm03de/lORx7fL3JVnYbp8T2zdYtPf9n5JcCPzNqNt8YbvdrkxyRZJNknwhA59WJTk1yYHTZd9KY4X9IMmjVnJMHbn+I7ZHkmcnuTDJ5UnOS7JV227kuH3pyO2t4bu6ytp96fokn23vw5lJNp5gH/njJP/VHjt/mOQpU30fRgzcl8+0dZ+W5MVJLm6fG7u37cZ9ro+xzo+29/P8JLPaZW9qt81VSe2IP+kAAApsSURBVM5qt9cmSX6aZIO2zePT9NpukOQp7fPz8iTfTbJT2+YRr7sT3LWZox+f9vrj7Y+j99sL0hwnf5DkJ0le0LbbKMkp7XPjiiQvapcfnuS4gW1wbpK9VrLdx9vnH5/k7CTXJZmXtqc3yaeSLGqfg//YLts7ydkD690nyZfb6fGOZf/crvtHSf51gm04oaq6lebH8o5KY9xtkOSudnte3j4Xdh/Yl17Rtjk8yVeSnNPuF0el6em+IsllSTZv94sfDtzGjkkupw9Vtd78AR8E/hU4Hvh74CCas4c8CngS8BvgoLbtTcD/bacPA85tp88BXt9OvwH4ykqWXw1s3U5v1v4/HDhuqrdHh+12IHDSwPymwM+Bx7bznwJeC8yi+Rn3Hdrlm0917T1uk+1pwvNz2/kLgNnt9JuBs4CZa/N2AP4MuHBg/jrg9cC3aE5buWW7H2wF7DXyHGnbPjwPHAtcAjya5heobgc2AGYDVwKPATYB/ht45xTd17sGHtdr2unDgRvb/X0j4Gc0Pw61BXDRwP7/buB9ox9r4D+A/Qf2jxPGue1zgOe304+jOWvSAcBn22Ubts+rx0z1vjWwnQ4cZz8Y95g6aj0Pb492X7gEmNXOH0xzSlSAa4DntdP/PPLYrA1/7b5UA4/tycA7J9hHvg8c0E5vBGw81fdh1H1ZBvxp+9he3t6fAHNY/no35nN9jPUV8Jp2+n20r4fAEwbafBB4Wzt9CvDKdnou8NF2+nxgx3b6OcC32+kVXnc7PD4T7Y8P77cD8yO1vBT4r3b674BT2umd2ufHRox67QfOBfZqp28Cthj1PNt+rH2e5vh6H/BHNM/Bb7E8u2ze/p/R1vfM9nG6fuA+fR7Yn3GOZcDmNL8MnYm24QT7y11jLPsNzbFiom1QwH7t9NnAN9vHYxfgynb54TSnRd6EJnPcARzZXvZvwNHt9HeAXdvpf6Ldlyb7b73piW69n6bHcDbwL8CewJeq6qGq+iXNRh90+sD/PdrpPWh2QGgOgHuuZPnFwKlJ3kSzU6+NrgZe3L5DfEFV3QF8A9g/zUdaLwO+CjwXuKiqfgpQVb+esorXjJ9V1WVjLH8xMK+qlsHaux2q6grgiWnG5e1CcxDcFTi9qh6sql8BFwK7DbG6r1XV76vqNuBWmoPpnsBXq+reqrqTJkxON+dX1R1VdR/Nm4gn0+znOwMXJ7mS5o3Fk9v2L0ry/SRXA38BPH1gXV8Y5zYuBj7W9tpt1u43Xwf+Ismjgf1onlf3Mn32rT0Zez9Y2TF10Mj2eBrwDOBb7fZ8D7BNks2ATarqkrbd58dYx3R3c1Vd3E7/J832WWEfSbIJTeg7G6Cq7quqe6am5HH9tKqurqqHgGtpnhtF8/qw/UC7sZ7roz3E8sd/ZLsAPKPtUb4aeA3Lnz+fYfnvSBwBnNL2mD4P+FK733ya5o0cDP+6O9bjM+b+OHCd0c/jL7f/L2f5dtiTJgdQVdfTvAF/6gR1rIofVNWNVfUgTUYZ2Yb/q+2FvYJm++3cPk7/Aby2fV7tQXOMGe9Y9juakP6ZJH9F8zseqytDtLmfJltAs19dWFUPsOI+9p2qurOqltKE6HMGrjPS7jPAEUlm0LwR6uX4sb6NRducpqdnA5p3hSt7UGuc6fHarLC8qo5M8hyaoHllkl2HL3d6qKqfJHk2zTvtDyX5Js2B5K3Ar4GFVXVnkjD+9lgX3T3O8nVpO5xJ07v4JOAMYFU/Yv79wPSDNMeeYQ6qU228ur9VVYcONkyyEXACzScSNyc5luY4M2LM/aWq/jnJ12ieX5cleXFVXZ/kAuAlNC8AI2/op8u+Nd5j1+UxHdkeAa6tqj0GL0zyB6tS2DQz+rEqxt5H1rbnwkMD8w/xyCwx1nNmZUa206k0Pc5XJTmcpseVqrq4HdrwQmBGVV2T5PHAb6tqhdfUsV53q+r2CW53cH7M/XHA6OfxyP0dvK/jPZ7LeOQw2o3GaTeMFWpPsgNNb/puVfWbJKcO3MYpNGHzPpo3usva1+wVjmUAaYbo7E3za9NH0bzhWyVJ/ohm+9zKxNvggTbww8A+VlUP5ZFj0IfZF8+iGYf9beDycR7/1ba+9USfCLwXOA34MPA94MA04/i2pH3CDjh44P+l7fQlNDsVNO+UvzfR8iRPqarvV9X7gNtoPg6+k+ajiLVCkj8E7qmq/6QZDvMsmo+JnkXzhbqRd+aXAi9sn8gk2XzNVzstfBM4cuRJv5ZvhzNo9uuDaAL1RTTj+WekGcf458APWLV9+ns0n2Zs1PYqvWzyyu7VZcDzs/x7EhsneSrLXwxua+/PUF8Iao8RV1fVh4FFNB//QrPtjwBeQPPLrzB99q3x9oOVHVPHcgMwK8keAGnGuj69qn4D3JnkuW27Q8Zdw/S13cj9Ag5l+evFI/aRqvodsCTJKwGSPDrt+Nx11KNY/vx4Ncu3yybAL9KMf37NqOt8jubN5Cnw8Db7aZJXwcPj9Hdpp8d63R3LWI/PmPtjx/t30Uj97bFhu3a9NwG7ts+PbYHdO6530O5JdkgzFvrgtvbH04T8O9rn334jjavqFuAWmp71U9vFYx7L2n1z02p+MO9omk8gV0l7fJhHM4SjmNxtMK7208PzaIabntLHbcB61BOd5DBgWVV9vu3ev4TmY5glNOPufkIzJu2Ogas9Osn3aZ7wI+/U3g6cnORdwFKWf8Q03vKPJNmR5p3p+cBVNOOjjmk/PvlQVY33Me908ac09+Mh4AHgzVX1YJovjh1O8xEQVbU0yVzgy+0T+1aa4TPrm8/QfHT3oyQPACcBx018lempqq5tP2r+n6r6RZovp+xBsx8X8H+q6pdJbgeWJbmK5gB9xRDrXphkfruun9EEyDsmvtbUa/fzw4HT2+EWAO9pP7E5ieYjxZuAhUOu8ug0Xzx6kGbIyNfb5d+kCQ7zq+r+dtl02bfG2w/Ooum9Gu+YuoKquj/Nlw8/kWRTmtelj9MMGfhr4KQkd9O8cZ/2+8coPwZen+TTNGP+PwX8AWPvI68DPp3k/TTH2VfRjMlfF91NM4zlcprHdKTD6r00+8zPaLbR4Bvz02jGSZ8+sOw1wKeSvIfmE+YzaPbJsV53x7LC47OS/XFYJ9B80fxqmp7Xw6vq90kuBn7a3rdrgB9OsI6VuZTmewJ/ShPaz257bK9oa72RZljLoNNoxkVfB+Mfy2g6Rb7afroW4B0da3tMm282oLn//wF8rL1sMrfBypwG/BXNsbQX6/0vFiZ5XFXdleQJND0pz2/H8knq2cDzb2OaF4K5VdXnQVU9m8xj6si62uljgK2q6m9WcrVpIcn2NF+ufcYUl7JOaIPtnKp63VTXsrZKc1aMK6rq36e6ljUhzdmeNq2q9/Z1G+tNT/QEzk0z0H5D4AMGaGmNOjHJzjRDIT5rgF4nTOYx9WVJ/p7mtepnNJ98aT2T5JM0QxP8fYVV1Pb6301z5pB1Xvup6VNYjbHcQ93O+t4TLUmSJHW1vn2xUJIkSVpthmhJkiSpI0O0JEmS1JEhWpIkSerIEC1JkiR1ZIiWJEmSOvr/AXsBLPtbpVYUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Model Results\", fontsize=20)\n",
    "plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
