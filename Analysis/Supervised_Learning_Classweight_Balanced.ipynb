{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "2  omg there s more on the ballot then just the p...          -0.7003   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...           0.5437   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         üò©  \n",
       "1            0.010101         üòä  \n",
       "2            0.000000         üò±  \n",
       "3            0.000000         üòä  \n",
       "4            0.021429         üòä  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2617.000000\n",
       "mean        0.596261\n",
       "std         0.307721\n",
       "min         0.000000\n",
       "25%         0.304228\n",
       "50%         0.690462\n",
       "75%         0.874311\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:00<00:00, 8197.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         üò©        0.000000  \n",
       "1            0.010101         üòä        0.111111  \n",
       "2            0.000000         üò±        0.000000  \n",
       "3            0.000000         üòä        0.000000  \n",
       "4            0.021429         üòä        0.037037  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:05<00:00, 476.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         üò©        0.000000   0.000000  \n",
       "1            0.010101         üòä        0.111111   0.010753  \n",
       "2            0.000000         üò±        0.000000   0.000000  \n",
       "3            0.000000         üòä        0.000000   0.000000  \n",
       "4            0.021429         üòä        0.037037   0.007576  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2617/2617 [00:00<00:00, 5180.34it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's!! gooooo to the zoooo!. Wazzzzuppppp!!!. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's!! goo to the zoo!. Wazzupp!!. AABBCC\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1487\n",
       "üò©     625\n",
       "üò°     335\n",
       "üò±     170\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2437905999235766\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2411157814291173)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1487\n",
       "üò©     625\n",
       "üò°     335\n",
       "üò±     170\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses balanced, see other notebook for resampled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# cry = df[df.top_emoji == 'üò©']\n",
    "# happy = df[df.top_emoji == 'üòä']\n",
    "# fear = df[df.top_emoji == 'üò±']\n",
    "# anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "# cry_downsampled = resample(cry,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number\n",
    "#                           random_state=seed) \n",
    "\n",
    "# happy_downsampled = resample(happy,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# fear_upsampled = resample(fear,\n",
    "#                           replace=True, \n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# anger_upsampled = resample(anger,\n",
    "#                           replace=True,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "\n",
    "# df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "# df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'], \n",
    "                 'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "            \n",
    "#             ('embedding', Pipeline([\n",
    "#                 ('selector', ItemSelector(key='tweet')),\n",
    "#                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "#             ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization','profanity','sentiment_score','exclamation_points'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 1,#0.9,\n",
    "#             'embedding': 1,\n",
    "            'stats': 1 #1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x126791d30>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (2093, 1488) - (524, 1488)\n",
      "CPU times: user 1e+03 ms, sys: 9.75 ms, total: 1.01 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    3.2s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "lr_clf.fit(train_vec, y_train)\n",
    "test_preds = lr_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Testing Accuracy: 0.7137\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('LogReg')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('LogReg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.88      0.90      0.89       298\n",
      "           üò°       0.42      0.37      0.40        67\n",
      "           üò©       0.55      0.55      0.55       125\n",
      "           üò±       0.36      0.38      0.37        34\n",
      "\n",
      "    accuracy                           0.71       524\n",
      "   macro avg       0.55      0.55      0.55       524\n",
      "weighted avg       0.71      0.71      0.71       524\n",
      "\n",
      "----------------------------------------\n",
      "[[267   7  22   2]\n",
      " [ 16  25  23   3]\n",
      " [ 17  21  69  18]\n",
      " [  4   6  11  13]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Linear SVC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.89      0.94      0.91       298\n",
      "           üò°       0.50      0.37      0.43        67\n",
      "           üò©       0.60      0.62      0.61       125\n",
      "           üò±       0.37      0.32      0.34        34\n",
      "\n",
      "    accuracy                           0.75       524\n",
      "   macro avg       0.59      0.56      0.57       524\n",
      "weighted avg       0.74      0.75      0.74       524\n",
      "\n",
      "----------------------------------------\n",
      "[[280   4  12   2]\n",
      " [ 14  25  24   4]\n",
      " [ 19  16  77  13]\n",
      " [  3   5  15  11]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't resample, f1 is better than accuracy -- look into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, class_weight='balanced')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=100, class_weight='balanced')\n",
    "svc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid\n",
      "Testing Accuracy: 0.7557\n"
     ]
    }
   ],
   "source": [
    "test_preds = svc.predict(test_vec)\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('SVC Grid')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2411157814291173),\n",
       " ('LogReg', 0.7137404580152672),\n",
       " ('Linear SVC', 0.75),\n",
       " ('SVC', 0.7557251908396947)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('SVC', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.88      0.94      0.91       298\n",
      "           üò°       0.58      0.37      0.45        67\n",
      "           üò©       0.61      0.63      0.62       125\n",
      "           üò±       0.36      0.35      0.36        34\n",
      "\n",
      "    accuracy                           0.76       524\n",
      "   macro avg       0.61      0.57      0.59       524\n",
      "weighted avg       0.74      0.76      0.75       524\n",
      "\n",
      "----------------------------------------\n",
      "[[280   3  14   1]\n",
      " [ 16  25  22   4]\n",
      " [ 18  12  79  16]\n",
      " [  4   3  15  12]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=250, random_state=seed,n_jobs=-1,verbose=1, class_weight='balanced')\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7729\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.89      0.94      0.91       298\n",
      "           üò°       0.64      0.37      0.47        67\n",
      "           üò©       0.58      0.75      0.66       125\n",
      "           üò±       0.86      0.18      0.29        34\n",
      "\n",
      "    accuracy                           0.77       524\n",
      "   macro avg       0.74      0.56      0.58       524\n",
      "weighted avg       0.78      0.77      0.75       524\n",
      "\n",
      "----------------------------------------\n",
      "[[280   3  15   0]\n",
      " [ 11  25  31   0]\n",
      " [ 22   8  94   1]\n",
      " [  3   3  22   6]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.6145\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.5954\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2411157814291173),\n",
       " ('LogReg', 0.7137404580152672),\n",
       " ('Linear SVC', 0.75),\n",
       " ('SVC', 0.7557251908396947),\n",
       " ('RFC', 0.7729007633587787),\n",
       " ('MNBayes', 0.6145038167938931),\n",
       " ('BerBayes', 0.5954198473282443)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.6813\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2411157814291173),\n",
       " ('LogReg', 0.7137404580152672),\n",
       " ('Linear SVC', 0.75),\n",
       " ('SVC', 0.7557251908396947),\n",
       " ('RFC', 0.7729007633587787),\n",
       " ('MNBayes', 0.6145038167938931),\n",
       " ('BerBayes', 0.5954198473282443),\n",
       " ('PassiveAgg', 0.6812977099236641)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "   y_train=='üòä', y_train=='üò©',y_train== 'üò°',y_train=='üò±'\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    1.75838926, 4.192, 7.82089552, 15.41176471\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.76030278,  4.186     ,  7.80970149, 15.38970588])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / np.array(list(y_train.value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A BASELINE XGB\n",
    "xg = xgb.XGBClassifier(n_jobs=-1)\n",
    "xg.fit(train_vec, y_train, eval_metric='mlogloss', sample_weight = np.select(conditions, choices, None))\n",
    "test_preds = xg.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Testing Accuracy: 0.7576\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('XGBoost')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.2411157814291173),\n",
       " ('LogReg', 0.7137404580152672),\n",
       " ('Linear SVC', 0.75),\n",
       " ('SVC', 0.7557251908396947),\n",
       " ('RFC', 0.7729007633587787),\n",
       " ('MNBayes', 0.6145038167938931),\n",
       " ('BerBayes', 0.5954198473282443),\n",
       " ('PassiveAgg', 0.6812977099236641),\n",
       " ('XGB', 0.7652671755725191)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('XGB', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.91      0.90      0.91       298\n",
      "           üò°       0.62      0.51      0.56        67\n",
      "           üò©       0.59      0.65      0.62       125\n",
      "           üò±       0.36      0.38      0.37        34\n",
      "\n",
      "    accuracy                           0.76       524\n",
      "   macro avg       0.62      0.61      0.61       524\n",
      "weighted avg       0.76      0.76      0.76       524\n",
      "\n",
      "---------------------------------------\n",
      "[[269   6  23   0]\n",
      " [ 10  34  17   6]\n",
      " [ 15  12  81  17]\n",
      " [  1   3  17  13]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('---------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('svc', svc), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #(\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "voting_clf.fit(train_vec, y_train)\n",
    "test_preds = voting_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "Testing Accuracy: 0.7519\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Voting')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Voting', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RFC', 0.7729007633587787),\n",
       " ('XGB', 0.7652671755725191),\n",
       " ('SVC', 0.7557251908396947),\n",
       " ('Voting', 0.7519083969465649),\n",
       " ('Linear SVC', 0.75),\n",
       " ('LogReg', 0.7137404580152672),\n",
       " ('PassiveAgg', 0.6812977099236641),\n",
       " ('MNBayes', 0.6145038167938931),\n",
       " ('BerBayes', 0.5954198473282443),\n",
       " ('Dummy', 0.2411157814291173)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAF6CAYAAADf+gS3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxdZX3v8c+XBATFmWiVQdCiiFaoRuoMalVQFL1ShdYiTpEqKrYOdFBRb6sWpypgRC+ivQqKI2AUFQUUwSZgGILADUghghIsggjK9Lt/POuYzeEkOSs5K+ckfN6v13mdNTx77d9aZ5+9v/vZz14rVYUkSZKkydtouguQJEmS1jeGaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRL0jqU5JQknlt0JZIckqSS7DbdtUjSqhiiJd1ldOFsdT+7TXedayPJZeP25/Yk1yU5M8lBSTae7hrXRLcvp0x3HZI0ZvZ0FyBJ0+Ddq1h32boqYmD/AfwGmAVsA/wv4CPAM4HnT2NdkrRBMERLusupqkOmu4Z14KNVddnYTJL3AouBPZPsWlWnTltlkrQBcDiHJK1EkgcneWeS05P8MsnNSa5M8oUkj5yg/bbdsIOjkzw8yReTXN0NqdhtJfexe3ebo1ay/m5Jrul+7ram+1JVS4Gx4Pz4Ce5nh67uK5L8Icmvuv18xARtH5jkg0kuSvK7JL/ppo9O8tCRdvt3+7b/SvZttUM0xrbRze46bqjKISPtXpDk5CRXdfVfmeTUJK9b3bGRpDVhT7QkrdzTgIOBHwBfAW4Atgf2Bl6Q5MlVdc4Et3sY8BPgYuDzwGbA9Su5j5OAS4CXJnlzVV03bv2LgfsDH6qqP6zl/qT7fcsdFia7A18FNgZOAJYCW9GGgDwvydOr6uyu7d2B02n7+N2ufYCHAHsBXwYuXcs6Ry2mDb95F/DfwNEj607papoHfBL4ZVfPNcADgMcArwCOmMJ6JAkwREu6CxrtwRzn91X1/pH57wMPrKrfjrv9TrQg+X5gjwm28xTgfVX1T6urpaoqyXzgUOBvgcPGNZnX/T5yddtala5Heddu9kcjy+8LHAPcCDytqi4YWfco2puBTwOP7RY/kxagP1pVbx53H5sAa9xbPpGqWgwsTvIu4LKVDMV5LXAzsFNVXT2upi2msh5JGmOIlnRX9K6VLL+OFowBGB/IRpafk+T7wLOTbFxVt4xr8itW/eXF8T4DvJcWBv8YokeC7w+q6uIe2wM4KMnoFwtfDNwd+GBVnTXSbj/gPsCBowEaoKqWJPlUt60dx62/afwdVtXNtDA7HW5lXA87QFVdMw21SLoLMERLusupqqy+VZPkecABwFxgC+78vLkFcNW4Zef0GXpRVb9O8iVgvyRPqqofd6vGeqHnT3ZbI940wbJDqmp8uH9i93unlfTQP7z7/UjgAtq46l8AByd5LLCA1iu/uKpuW4M6p8LngQ8BS5J8savx9KpaPk31SLoLMERL0kokeSPtVHHX0sb/Xk4b9lDAC4GdmHj4wi/X4O6OoPUKvxb4cfclwpcDVwNfX4PtbVdVlyXZFNiZFsTfleTSqvrPkXb3736/ZjXb2xygqq5P8gRaT/sLgOd0669JcgTwvyfomR9UVX04yTXA64A3AgcBleRU4K1VtWhd1iPprsEQLUkTSDKbFhR/CTy2qq4at/6JE96w6X1Fwqr6SZKzgZckOYg21vr+wAe6YRJrpKp+D5yZZA/gQuATSU6uqiu7JmNfZNypqs6d5DaXAa9KEmBH4BnA64F30s769I6u6e3d7zu91iS5z5rszypq+hzwuW67TwJeBLwSOCnJI1c2NEeS1pSnuJOkiW1BGyv84wkC9Oas+KLdVPoEsCmtR3oeLYx/aio23O3DvwH34I7jtc/sfj91DbZZVbWkqj4OPKtb/MKRJtd2v7ee4OZze97d7bTx3aur6TdVtaCqXkM7k8f9WIN9k6TVMURL0sSupg3deFwXmgHoLpv9H7SQPdW+QOsZfhvtC4XfrapLpnD7H6d96XH/JNt3yz5Du7Lhu5LsMv4GSTYaPcd1kkcn2XaCbT+w+33jyLJFtPD7192p8ca2cT/g33vW/msmDuNj59qe6JPVB0xQkyRNCYdzSLrLWcUp7gC+XlWLq+r2JB+jnSf6vCTfADYBnk7r3fxBNz1lqurGJJ+ljeuFdu7jqd7++2mX/34PsG/3pca9ga/Rhn2cDCyhhd9taF88vD+thxzgL4EPJ/kxbXjI1bRzSu/V3ebQkfu7KsnnaafuW5zkm8C9gOcCpwF/3qP8k4F9kpwAnEU7G8dpVXUacCzw+yQ/ol22PbTe58d3bb/X434kaVIM0ZLuilZ2ijtoIWxxN/0OYDnwatoX/q6jfcHwX+h3Crs+jqKF6KuA4wfY/nzgrbSLu7yvqs6tqpOTPAZ4C+2Lgk+lnaruStq5sr8ycvuTgI/SLkSzFy0UX0U7Lh8eObPImNfQer/3pY2bvhz4GC1sv6RH3W+iDW95Ji2Eb0T7G5xGe6PzHNoQm+cCv6ddmOXtwCfW9RcdJd01pKr3918kSQPpLpH9GdpZLt6xmuaSpGliiJakGaIb13s27ZzM23VnwZAkzUAO55CkaZbkKbQvEu4G/BlwmAFakmY2Q7QkTb+/pI3T/h/aKe3eNr3lSJJWx+EckiRJUk+eJ1qSJEnqab0bzrHFFlvUtttuO91lSJIkaQN31llnXVNVcyZaN2iITrI77cpes4BPV9X7x62/N/B/aSf0nw18sKo+s6ptbrvttixatGigiiVJkqQmyX+vbN1gwzmSzAIOB/YAdgT2TbLjuGavBy6oqp1o30r/UJJNhqpJkiRJmgpDjoneBVhaVZdW1c20y7LuNa5NAfdMEmBz2jfTbx2wJkmSJGmtDRmitwSuGJlf1i0bdRjtogJXAucBb6qq28dvKMm8JIuSLFq+fPlQ9UqSJEmTMmSIzgTLxp9P7znAYuDBwM7AYUnudacbVR1ZVXOrau6cOROO7ZYkSZLWmSFD9DJg65H5rWg9zqNeAXy1mqXAz4EdBqxJkiRJWmtDhuiFwPZJtuu+LLgPcPy4NpcDzwRI8kDgEcClA9YkSZIkrbXBTnFXVbcmORA4iXaKu6OqakmSA7r184H3AkcnOY82/OPtVXXNUDVJkiRJU2HQ80RX1QJgwbhl80emrwSePWQNkiRJ0lTzst+SJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9TTo2TnWtce99XPTXcKUO+vQ/aa7BEmSJI1jT7QkSZLUkyFakiRJ6mmDGs6hFS5/z59NdwlTbpt3njfdJUiSJAH2REuSJEm9GaIlSZKkngzRkiRJUk+OidYG78kff/J0lzDlTn/D6dNdgiRJd2n2REuSJEk92RMt3YWc+rRdp7uEKbfraaeu0e0O+4cTpriS6Xfgh54/3SVI0l2GPdGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6mj3dBUiSpte/vmzv6S5hyv3z//3ydJcgaQNnT7QkSZLU06AhOsnuSS5KsjTJwROsf2uSxd3P+UluS3K/IWuSJEmS1tZgITrJLOBwYA9gR2DfJDuOtqmqQ6tq56raGfhH4NSq+p+hapIkSZKmwpA90bsAS6vq0qq6GTgW2GsV7fcFjhmwHkmSJGlKDBmitwSuGJlf1i27kyR3B3YHvrKS9fOSLEqyaPny5VNeqCRJktTHkCE6EyyrlbR9PnD6yoZyVNWRVTW3qubOmTNnygqUJEmS1sSQIXoZsPXI/FbAlStpuw8O5ZAkSdJ6YsgQvRDYPsl2STahBeXjxzdKcm9gV+AbA9YiSZIkTZnBLrZSVbcmORA4CZgFHFVVS5Ic0K2f3zV9EfCdqvrdULVIkiRJU2nQKxZW1QJgwbhl88fNHw0cPWQdkiRJ0lTyioWSJElST4ZoSZIkqadBh3NIkrQ++dm/fn+6S5hyj/znZ0x3CdIGyZ5oSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktTT7OkuQJIkzTyHHHLIdJcw5TbEfdL0sSdakiRJ6mnQEJ1k9yQXJVma5OCVtNktyeIkS5KcOmQ9kiRJ0lQYbDhHklnA4cCzgGXAwiTHV9UFI23uAxwB7F5Vlyd5wFD1SJIkSVNlyJ7oXYClVXVpVd0MHAvsNa7NXwNfrarLAarq6gHrkSRJkqbEkCF6S+CKkfll3bJRDwfum+SUJGcl2W/AeiRJkqQpMeTZOTLBsprg/h8HPBPYDDgjyZlVdfEdNpTMA+YBbLPNNgOUKkmSJE3ekD3Ry4CtR+a3Aq6coM23q+p3VXUNcBqw0/gNVdWRVTW3qubOmTNnsIIlSZKkyRiyJ3ohsH2S7YBfAPvQxkCP+gZwWJLZwCbAXwAfGbAmSZKkXr503C7TXcKUe8lf/dd0l7DeGyxEV9WtSQ4ETgJmAUdV1ZIkB3Tr51fVz5J8GzgXuB34dFWdP1RNkiRJ0lQY9IqFVbUAWDBu2fxx84cChw5ZhyRJkjSVvGKhJEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4OG6CS7J7koydIkB0+wfrck1yVZ3P28c8h6JEmSpKkwe6gNJ5kFHA48C1gGLExyfFVdMK7pD6tqz6HqkCRJkqbakD3RuwBLq+rSqroZOBbYa8D7kyRJktaJIUP0lsAVI/PLumXjPTHJOUm+leRRA9YjSZIkTYnBhnMAmWBZjZs/G3hIVd2Q5LnA14Ht77ShZB4wD2CbbbaZ6jolSZKkXobsiV4GbD0yvxVw5WiDqrq+qm7ophcAGyfZYvyGqurIqppbVXPnzJkzYMmSJEnS6g0ZohcC2yfZLskmwD7A8aMNkvxJknTTu3T1/HrAmiRJkqS1Nthwjqq6NcmBwEnALOCoqlqS5IBu/Xxgb+DvktwK3ATsU1Xjh3xIkiRJM8qQY6LHhmgsGLds/sj0YcBhQ9YgSZIkTTWvWChJkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKknlYbopPsmcSwLUmSJHUmE473Af5fkn9P8sihC5IkSZJmutWG6Kp6GfDnwCXAZ5KckWReknsOXp0kSZI0A01qmEZVXQ98BTgWeBDwIuDsJG8YsDZJkiRpRprMmOjnJ/ka8H1gY2CXqtoD2Al4y8D1SZIkSTPO7Em0+SvgI1V12ujCqroxySuHKUuSJEmauSYTot8FXDU2k2Qz4IFVdVlVnTxYZZIkSdIMNZkx0ccBt4/M39YtkyRJku6SJhOiZ1fVzWMz3fQmw5UkSZIkzWyTCdHLk7xgbCbJXsA1w5UkSZIkzWyTGRN9APD5JIcBAa4A9hu0KkmSJGkGW22IrqpLgCck2RxIVf12+LIkSZKkmWsyPdEkeR7wKGDTJABU1XsGrEuSJEmasSZzsZX5wEuBN9CGc/wV8JCB65IkSZJmrMl8sfBJVbUfcG1VvRt4IrD1sGVJkiRJM9dkQvTvu983JnkwcAuw3XAlSZIkSTPbZMZEn5DkPsChwNlAAZ8atCpJkiRpBltliE6yEXByVf0G+EqSE4FNq+q6dVKdJEmSNAOtcjhHVd0OfGhk/g8GaEmSJN3VTWZM9HeSvDhj57aTJEmS7uImE6L/HjgO+EOS65P8Nsn1k9l4kt2TXJRkaZKDV9Hu8UluS7L3JOuWJEmSps1krlh4zzXZcJJZwOHAs4BlwMIkx1fVBRO0+wBw0prcjyRJkrSurTZEJ3naRMur6rTV3HQXYGlVXdpt51hgL+CCce3eAHwFePxqq5UkSZJmgMmc4u6tI9Ob0sLxWcAzVnO7LYErRuaXAX8x2iDJlsCLum2tNEQnmQfMA9hmm20mUbIkSZI0nMkM53j+6HySrYF/n8S2J/oiYo2b/yjw9qq6bVXfW6yqI4EjAebOnTt+G5IkSdI6NZme6PGWAY+eZLvRy4NvBVw5rs1c4NguQG8BPDfJrVX19TWoS5IkSVonJjMm+uOs6EHeCNgZOGcS214IbJ9kO+AXwD7AX482qKo/Xj48ydHAiQZoSZIkzXST6YleNDJ9K3BMVZ2+uhtV1a1JDqSddWMWcFRVLUlyQLd+/poULEmSJE23yYToLwO/r6rboJ2SLsndq+rG1d2wqhYAC8YtmzA8V9X+k6hFkiRJmnaTudjKycBmI/ObAd8bphxJkiRp5ptMiN60qm4Ym+mm7z5cSZIkSdLMNpkQ/bskjx2bSfI44KbhSpIkSZJmtsmMiT4IOC7J2OnpHgS8dLiSJEmSpJltMhdbWZhkB+ARtAuoXFhVtwxemSRJkjRDrXY4R5LXA/eoqvOr6jxg8ySvG740SZIkaWaazJjo11TVb8Zmqupa4DXDlSRJkiTNbJMJ0Ruluy43tPNEA5sMV5IkSZI0s03mi4UnAV9KMp92+e8DgG8NWpUkSZI0g00mRL8dmAf8He2LhT+lnaFDkiRJukta7XCOqrodOBO4FJgLPBP42cB1SZIkSTPWSnuikzwc2AfYF/g18EWAqnr6uilNkiRJmplWNZzjQuCHwPOrailAkjevk6okSZKkGWxVwzleDPwS+EGSTyV5Jm1MtCRJknSXttIQXVVfq6qXAjsApwBvBh6Y5BNJnr2O6pMkSZJmnMl8sfB3VfX5qtoT2ApYDBw8eGWSJEnSDDWZi638UVX9T1V9sqqeMVRBkiRJ0kzXK0RLkiRJMkRLkiRJvRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktTToCE6ye5JLkqyNMnBE6zfK8m5SRYnWZTkKUPWI0mSJE2F2UNtOMks4HDgWcAyYGGS46vqgpFmJwPHV1UleQzwJWCHoWqSJEmSpsKQPdG7AEur6tKquhk4FthrtEFV3VBV1c3eAygkSZKkGW7IEL0lcMXI/LJu2R0keVGSC4FvAq+caENJ5nXDPRYtX758kGIlSZKkyRoyRGeCZXfqaa6qr1XVDsALgfdOtKGqOrKq5lbV3Dlz5kxxmZIkSVI/Q4boZcDWI/NbAVeurHFVnQY8LMkWA9YkSZIkrbUhQ/RCYPsk2yXZBNgHOH60QZI/TZJu+rHAJsCvB6xJkiRJWmuDnZ2jqm5NciBwEjALOKqqliQ5oFs/H3gxsF+SW4CbgJeOfNFQkiRJmpEGC9EAVbUAWDBu2fyR6Q8AHxiyBkmSJGmqecVCSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1NPs6S5AkiRJ64edvnzSdJcw5c7Z+zlrdDt7oiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLU06AhOsnuSS5KsjTJwROs/5sk53Y/P06y05D1SJIkSVNhsBCdZBZwOLAHsCOwb5IdxzX7ObBrVT0GeC9w5FD1SJIkSVNlyJ7oXYClVXVpVd0MHAvsNdqgqn5cVdd2s2cCWw1YjyRJkjQlhgzRWwJXjMwv65atzKuAb020Ism8JIuSLFq+fPkUlihJkiT1N2SIzgTLasKGydNpIfrtE62vqiOram5VzZ0zZ84UlihJkiT1N3vAbS8Dth6Z3wq4cnyjJI8BPg3sUVW/HrAeSZIkaUoM2RO9ENg+yXZJNgH2AY4fbZBkG+CrwN9W1cUD1iJJkiRNmcF6oqvq1iQHAicBs4CjqmpJkgO69fOBdwL3B45IAnBrVc0dqiZJkiRpKgw5nIOqWgAsGLds/sj0q4FXD1mDJEmSNNW8YqEkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPg4boJLsnuSjJ0iQHT7B+hyRnJPlDkrcMWYskSZI0VWYPteEks4DDgWcBy4CFSY6vqgtGmv0P8EbghUPVIUmSJE21IXuidwGWVtWlVXUzcCyw12iDqrq6qhYCtwxYhyRJkjSlhgzRWwJXjMwv65ZJkiRJ67UhQ3QmWFZrtKFkXpJFSRYtX758LcuSJEmS1s6QIXoZsPXI/FbAlWuyoao6sqrmVtXcOXPmTElxkiRJ0poaMkQvBLZPsl2STYB9gOMHvD9JkiRpnRjs7BxVdWuSA4GTgFnAUVW1JMkB3fr5Sf4EWATcC7g9yUHAjlV1/VB1SZIkSWtrsBANUFULgAXjls0fmf4lbZiHJEmStN7wioWSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9GaIlSZKkngzRkiRJUk+GaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPRmiJUmSpJ4M0ZIkSVJPhmhJkiSpJ0O0JEmS1JMhWpIkSerJEC1JkiT1ZIiWJEmSejJES5IkST0ZoiVJkqSeDNGSJElST4ZoSZIkqSdDtCRJktSTIVqSJEnqyRAtSZIk9WSIliRJknoyREuSJEk9DRqik+ye5KIkS5McPMH6JPlYt/7cJI8dsh5JkiRpKgwWopPMAg4H9gB2BPZNsuO4ZnsA23c/84BPDFWPJEmSNFWG7IneBVhaVZdW1c3AscBe49rsBXyumjOB+yR50IA1SZIkSWttyBC9JXDFyPyyblnfNpIkSdKMkqoaZsPJXwHPqapXd/N/C+xSVW8YafNN4H1V9aNu/mTgbVV11rhtzaMN9wB4BHDRIEX3swVwzXQXMUN4LFbwWKzgsVjBY9F4HFbwWKzgsVjBY7HCTDkWD6mqOROtmD3gnS4Dth6Z3wq4cg3aUFVHAkdOdYFrI8miqpo73XXMBB6LFTwWK3gsVvBYNB6HFTwWK3gsVvBYrLA+HIshh3MsBLZPsl2STYB9gOPHtTke2K87S8cTgOuq6qoBa5IkSZLW2mA90VV1a5IDgZOAWcBRVbUkyQHd+vnAAuC5wFLgRuAVQ9UjSZIkTZUhh3NQVQtoQXl02fyR6QJeP2QNA5pRw0ummcdiBY/FCh6LFTwWjcdhBY/FCh6LFTwWK8z4YzHYFwslSZKkDZWX/ZYkSZJ6MkRPIMltSRYnOT/JCUnu0y3fNslN3bqxn026dXskWZTkZ0kuTPLB6d2LtZNk6yQ/T3K/bv6+3fxDkmyf5MQklyQ5K8kPkjyta7d/kuXdsVmS5MtJ7j69ezM1kvxzt0/ndvv3rSTvG9dm5yQ/66Y3T/LJ7jgtSXJakr+Ynur7S3JKkueMW3ZQkiNW0v6fxs3/eMj61laSGyZYdkCS/dZxHXsm+WmSc5JckOS1SXZLcsa4drOT/GrsglRJ3tI915zf3XbQuic6Xmuwjd2SXNft74x7nhz33H/cVD13JVkw9jqyhrd/UZJKssNU1LMWdVSS/xyZn90935/Yze+f5PYkjxlpc36Sbbvpy5Kc1x3j85KMvwDbemPksXJOkrOTPGkNtrHBHI9RI8dmSXd8/j7Jhpk3q8qfcT/ADSPTnwX+uZveFjh/gvaPBi4BdujmZwOvm+79mILj8DbgyG76k8A/ApsCFwMvGLf/+3fT+wOHjaz7AvCK6d6XKTgWTwTOAO7WzW8B7ApcOq7d+4F3dNPHAu8DNurmHwo8b7r3pcc+vxb4zLhlZwJPXUn7G4asZ4D9W+f1Ahl7PHTzG9NO67lVN3832rnwN6JdiGrbkba7Ayd30wfQvrR9r27+3sDLZ/rxAnYDTuymNwMuBJ483Y+FifYR+Dzw99NdU1fLl4AfAodM9/EBfgps1s3vASwe+ZvuD1wOfHHkNuePPY6By4AtuulHAP893cd2ih4rzwFO7XHbdP/jG8zxWMWxeQDwPeDd013XED8b5juDqXUGq7+K4tuAf62qC6GdmaSqJuytW898BHhCkoOApwAfAv4GOKOq/ni6wqo6v6qOHn/jJLOBewDXrptyB/Ug4Jqq+gNAVV1TVacCvxnXu/wS4NgkDwP+AviXqrq9u82lVfXNdV34WvgysGeSu0H7JAZ4MLBV12tyfpIPdOveD2zW9T58vlt2Q/d7t65X+8td7+Pnk6Rb99xu2Y+SfGysR2u6JDkkyVu66VOSfCDJfyW5OArawXQAAAupSURBVMlTu+WzkhyaZGH3qcRru+WbJzm565X6Y69S2idYP+t68M/mjufGvyftTfevAarqD1V1UfeYOQ546UjbfYBjuul/or1Rv7673XVV9dmBDstKdZ+8nNkdh68luW+3/PHdsjO6Y3X++NtW1U20ALZld5tnd+3P7nqBN++WT9dj5IfAnyZ5fpKfdL3n30vywK6uXbPiE8mfJrlnkgelfeI01ps99pi5LMkW3ePpdWN30D3e/qGbfuvIY+rdI202B54MvIr2GBhbvlGSI7revhPTerv37tYNecy+BTyvm96XFY/JMScCj0ryiNVs516MvDYk+XraJ5tL0i6wRpJXJfnISJvXJPlwN/2y7n9zcdonfrO6n6O7Y39ekjev5b5O1vh9udPfcjXPAxNtY30+Hn9UVVfTLpZ3YJr9kxw2sg8nJtmtm76h+x85q/tf2yXtefjSJC/o2uzfHZsT0j4dPzCtp/un3XPR/ZI8LMnZI/exfZKzGMJ0p/iZ+EP3Lop2ar7jgN27+W2BsSf+xcDh3fKzgZ2mu+6BjsVzgAKe1c1/GHjTKtrvDyzvjs+vaC9Es6Z7P6bgOGze7dPFwBHArt3ytwIf6aafACzspl8AfG26656C/f4msFc3fTDwKVpP0xxa+Ps+8MJu/Q3jbjv2f7QbcB3tYkob0d6YPoX2qcYVwHZdu2PoerTW0b7dqWcVOAR4Szd9CvChbvq5wPe66Xm0N0fQeo4XAdt1x2OsZ3gL2qk70z1v3A48YSV1fBq4utv/v2HFJxePB346cj9XA/elBe9rp+GxMNHxOnfkf+E9wEe76fOBJ3XT76f7BI879kTfFzgL+JPueJ0G3KNb93bgnev6MTLymJ0NfAP4u67OsS/hv3rkMXECXS867flhNvAPrPjkchZwz276sm4f/5yRHkvgAmAb4Nm0MxGM9VCeCDyta/My4P900z8GHttN7007+9VG3TG8tls22DGj9UQ/hvYGe1Pac+Lo33R/4DBgP+CzI4+FbUeOw3ndshuBPUe2fb/u92bd+vvTOmEuATYe2f8/Ax7ZHf+x5Ud09/k44Lsj27zPgI+V27r9v5D2/Pa4bvmEf0smeB7YkI7HRP9H45ZdCzyQO39afSKwWzddwB7d9NeA79A+rdsJWDzyGFtKex6c0x37A7p1HwEO6qZ/AOzcTf8b8IYh9tWe6IltlmQxrXfofsB3R9ZdUlU7dz/r6+n5+tgDuIo2ZONOut6n85N8dWTxF6tqZ9oT+3m0oLleq6obaE9I82hvEr6YZH/akI2908Z7jfYUbiiOYUXv1z60q4yeUlXLq+pW2kfeT5vEdv6rqpZV62FdTHtB2YE2HObnI/c104w9rs+i1QztRXK/7jniJ7QXt+1pL5r/luRc2seXW9JeNKB9THvmRHdQVa8Gngn8F/AW4Khu+UJg865Hbw/gzKq6trufaT+tUpJ7016UT+0WfRZ4WtrY33tW1diY+C+Mu+lTu2P0S1r4+iXtDeiOwOndcX058BDW/WNk7Ll/Ee3N4v+hvfk7KcnYc9mjuranAx9O8kbacbiVdpGxVyQ5BPizqvrt6Mar6qfAA5I8OMlOtDdDl9MeU8+mDZU4m7bf23c325f2PEP3e99u+inAcVV1e3cMf9AtH/SYVdW5tP+FfRl3CtsRX6B9irndBOueXlWPpoW/w7qedoA3JjmHNmRsa2D7qvod7Y36nmnjwTeuqvNo/y+PAxZ2f69n0obLXQo8NMnHk+wOXL/2e7xSN3U5YAfaUKvPJQmr/ltO9DywoRyP1ckk2twMfLubPo/2hvOWbnrbkXY/qKrfVtVyWog+YeQ2Y+0+TftfnEX7RG/889CUGPQ80euxm6pq5+5F4kTauaw/tor2S2gP4HPWRXHrSpKdgWfRXuB+lORY2r7+MTRV1YuSzAXu9AWhqqokJwBvoPVGrdeq6jZa7+Qp3Qvqy6vq6CSX0cZHv5g2dhracdopyUZdcFxffZ0WFB5L6xE5B3jYGmznDyPTt9GeeybzpDrdxuoeqxla3W+oqpNGG3ZvqubQeqRu6R4Xm3arf7eqO+leCM9L+9LWz2m9LdBC0z60nqZjurbXJ/ldkodW1aVrvmuDWd3f9YdVtWeSh9OeV77W3ea7VbXvaMMkfz5UkStxU9cBMFrDx4EPV9Xx3cfOhwBU1fuTfJP2KcWZSf6yqk5L+5L184D/THJoVX1u3H18mdZj/CesCMcB3ldVnxx33/cHngE8OknRercrydtY+XFeF/9Xx9Oe83ejvYm8g2oXW/sQ7ROFCVXVJUl+BeyY9gXOvwSeWFU3JjmFFf87n6YNX7oQ+Ey3LLSe7n8cv93uzclzaK/bLwFeuSY72EdVnZFkC9r//8r+ltuyiueBDel4TFDDQ2nPoVcDt3LHk1psOjJ9S3Vdx7Re+7Hhk7enDQ8dM/p6cvvI/O2seJ7+CvAu2puOs6rq11OwK3diT/QqVNV1wBuBtyTZeBVNDwX+qXtRGBur9vfrosahdO+oP0H7aORy2j5+kPZu7slj45M6q/oG+1NoHz+t15I8Isn2I4t2Bv67mz6G9jHSJVW1DNoTIq03693dsRwbl7Veffu664E/hdY7egyt53XXtPGds2i9UWM9kbes5v9kvAtpvSTbdvMvXXnTGeUk4O/G9jXJw5Pcg/blvqu7AP10Wk/qKqWNo95tZNHo4wraMX8ZLUgdP7L8fcDhSe7Vbede6cZNrivd8+O16cb9An9L6zm6Fvhtkid0y/dZye0vpu3H22m9bU9O8qcASe7ePZ/OhMfIvYFfdNMvH1uY5GFVdV5VfYD2v75DkofQHgOfovViP3aC7Y29MdqbFqihPaZemRXjwLdM8oCuzeeq6iFVtW1VbU17k/UU4EfAi7vXmwfSAi2sm2N2FPCe7s3fyhxNC4JzJlrZ7d92tMf7vWm98jd2Paxjjx2q6ie0nti/ZkWv+sm0TwAf0G3rfmlnjtqCNhzqK8A7mPj4T7mu5lm0T69X9rdc3TY2mOMxKskcYD5tCEfRhrDs3D1utwZ2GeJ+q+r3tL/FJ1jxZmPK2RO9GlX10+4jlX1o43snanNu2pfvjuneQRZtLOn67DXA5VU1NpTlCFrv2C7AnrTeyY/Sxj3/FvjfI7d9aZKn0N6kLWNFr9r6bHPg42kfVd9KG5M1FlqOA/6D1uM+6tW0L2MuTXIj7Ql2fRzacgxtWMM+VXVVkn+kfXQcYEFVfaNrdyRwbpKzq+pvVrfRqrop7UtW305yDW04w7p09yTLRuY/PMnbfZr2keHZ3Ruk5cALaUNbTkiyiBVjJVcnwNuSfJL2fYvfMfL/UlUXdI+ds7qPcsd8gvaYXJjkFuAW2mNtSBMdr5cD87vnvUuBV3TrXgV8KsnvaG/CrlvJNufThrBsTtvvY9J9kZU27vziaX6MQOt5Pi7JL2hhf2yIwkHdm6XbaGObv0V7nXhr9ze5gTYu9Q6qakmSewK/qKqrumXfSfJI4IzuPfcNtDdP+3LnT/G+QgtQr6d9bH8+7bsaPwGuWxf/V11nwX+sps3NST42QbsfJLmNNtb14Kr6VZJvAwekDfO5iHacR32JNr712m7bFyT5F+A7aUPpbqEdj5uAz2TF6dTu1DM7hcaG/kD7P35592nlyv6Wt61kOxvK8Rg1dmw2pr1e/icrnl9Pp70RHBsLfvaEW5ganwf+F21s9SC8YqGkaZNk86q6oQujhwP/r6o+srrbaWYb+7t20wcDD6qqN63NtnyM3NnIsbk/LSw/uap+uaEds7Szi3ykqk6e7lpmAo/H5KSdaeneVfWOoe7D4RySptNruh6LJbSPMD+5mvZaPzwv3WnegKdyx0+q+vIxsnIndsfmh8B7uy8YwgZyzJLcJ8nFtLHqd/nA6PGYvLTvWuzHaj4xWev7sSdakiRJ6seeaEmSJKknQ7QkSZLUkyFakiRJ6skQLUmSJPVkiJYkSZJ6MkRLkiRJPf1/KsD2i2f3n/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
