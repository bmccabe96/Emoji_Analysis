{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>-0.7003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.5437</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...          -0.7447   \n",
       "1  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "2  omg there s more on the ballot then just the p...          -0.7003   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...           0.5437   \n",
       "\n",
       "   exclamation_points top_emoji  \n",
       "0            0.000000         üò©  \n",
       "1            0.010101         üòä  \n",
       "2            0.000000         üò±  \n",
       "3            0.000000         üòä  \n",
       "4            0.021429         üòä  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont want to vote for pedophile biden im sorry what\n",
      "dont want vote pedophile biden im sorry\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2771.000000\n",
       "mean        0.593511\n",
       "std         0.307254\n",
       "min         0.000000\n",
       "25%         0.301172\n",
       "50%         0.690462\n",
       "75%         0.871406\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:00<00:00, 8024.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  \n",
       "0            0.000000         üò©        0.000000  \n",
       "1            0.010101         üòä        0.111111  \n",
       "2            0.000000         üò±        0.000000  \n",
       "3            0.000000         üòä        0.000000  \n",
       "4            0.021429         üòä        0.037037  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:05<00:00, 493.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>exclamation_points</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont want to vote for pedophile biden im sor...</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò©</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.351818</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.010753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg there s more on the ballot then just the p...</td>\n",
       "      <td>0.148382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üò±</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.729035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biden will WIN Trump and DeJoy have cheated!! ...</td>\n",
       "      <td>0.771566</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  i dont want to vote for pedophile biden im sor...         0.126140   \n",
       "1  I ll kidnap 1000 children before I let this co...         0.351818   \n",
       "2  omg there s more on the ballot then just the p...         0.148382   \n",
       "3       Am√©ricaniseUnTitre The Trump Tower Infernale         0.729035   \n",
       "4  Biden will WIN Trump and DeJoy have cheated!! ...         0.771566   \n",
       "\n",
       "   exclamation_points top_emoji  capitalization  profanity  \n",
       "0            0.000000         üò©        0.000000   0.000000  \n",
       "1            0.010101         üòä        0.111111   0.010753  \n",
       "2            0.000000         üò±        0.000000   0.000000  \n",
       "3            0.000000         üòä        0.000000   0.000000  \n",
       "4            0.021429         üòä        0.037037   0.007576  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2771/2771 [00:00<00:00, 5240.93it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's!! gooooo to the zoooo!. Wazzzzuppppp!!!. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's!! goo to the zoo!. Wazzupp!!. AABBCC\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1562\n",
       "üò©     663\n",
       "üò°     363\n",
       "üò±     183\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25261638397690367\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24936845904005775)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üòä    1562\n",
       "üò©     663\n",
       "üò°     363\n",
       "üò±     183\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses class_weight = balanced, see other notebook for resampled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# cry = df[df.top_emoji == 'üò©']\n",
    "# happy = df[df.top_emoji == 'üòä']\n",
    "# fear = df[df.top_emoji == 'üò±']\n",
    "# anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "# cry_downsampled = resample(cry,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number\n",
    "#                           random_state=seed) \n",
    "\n",
    "# happy_downsampled = resample(happy,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# fear_upsampled = resample(fear,\n",
    "#                           replace=True, \n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# anger_upsampled = resample(anger,\n",
    "#                           replace=True,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "\n",
    "# df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "# df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity','exclamation_points']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'], \n",
    "                 'sent': row['sentiment_score'], 'excla': row['exclamation_points']} for _, row in data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "            \n",
    "#             ('embedding', Pipeline([\n",
    "#                 ('selector', ItemSelector(key='tweet')),\n",
    "#                 (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "#             ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization','profanity','sentiment_score','exclamation_points'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 1,#0.9,\n",
    "#             'embedding': 1,\n",
    "            'stats': 1 #1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x11d848d30>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score',\n",
       "                                                                                    'exclamation_points'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (2216, 1527) - (555, 1527)\n",
      "CPU times: user 940 ms, sys: 4.7 ms, total: 945 ms\n",
      "Wall time: 944 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    2.1s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "lr_clf.fit(train_vec, y_train)\n",
    "test_preds = lr_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Testing Accuracy: 0.7153\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('LogReg')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('LogReg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.88      0.88      0.88       313\n",
      "           üò°       0.41      0.40      0.40        73\n",
      "           üò©       0.59      0.59      0.59       133\n",
      "           üò±       0.36      0.36      0.36        36\n",
      "\n",
      "    accuracy                           0.72       555\n",
      "   macro avg       0.56      0.56      0.56       555\n",
      "weighted avg       0.71      0.72      0.71       555\n",
      "\n",
      "----------------------------------------\n",
      "[[276  17  20   0]\n",
      " [ 17  29  19   8]\n",
      " [ 19  20  79  15]\n",
      " [  1   5  17  13]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Linear SVC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.87      0.92      0.89       313\n",
      "           üò°       0.38      0.33      0.35        73\n",
      "           üò©       0.64      0.62      0.63       133\n",
      "           üò±       0.42      0.36      0.39        36\n",
      "\n",
      "    accuracy                           0.73       555\n",
      "   macro avg       0.58      0.56      0.57       555\n",
      "weighted avg       0.72      0.73      0.73       555\n",
      "\n",
      "----------------------------------------\n",
      "[[287  15  11   0]\n",
      " [ 21  24  21   7]\n",
      " [ 21  18  83  11]\n",
      " [  1   7  15  13]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=15, class_weight='balanced')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=15, class_weight='balanced', kernel='rbf', gamma='scale')\n",
    "svc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid\n",
      "Testing Accuracy: 0.7604\n"
     ]
    }
   ],
   "source": [
    "test_preds = svc.predict(test_vec)\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('SVC Grid')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24936845904005775),\n",
       " ('LogReg', 0.7153153153153153),\n",
       " ('Linear SVC', 0.7333333333333333),\n",
       " ('SVC', 0.7603603603603604)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('SVC', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.88      0.93      0.90       313\n",
      "           üò°       0.46      0.36      0.40        73\n",
      "           üò©       0.62      0.66      0.64       133\n",
      "           üò±       0.67      0.44      0.53        36\n",
      "\n",
      "    accuracy                           0.76       555\n",
      "   macro avg       0.66      0.60      0.62       555\n",
      "weighted avg       0.75      0.76      0.75       555\n",
      "\n",
      "----------------------------------------\n",
      "[[292   5  16   0]\n",
      " [ 18  26  24   5]\n",
      " [ 22  20  88   3]\n",
      " [  1   6  13  16]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=250, random_state=seed,n_jobs=-1,verbose=1, class_weight='balanced')\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7387\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.88      0.91      0.89       313\n",
      "           üò°       0.51      0.34      0.41        73\n",
      "           üò©       0.54      0.68      0.61       133\n",
      "           üò±       0.64      0.25      0.36        36\n",
      "\n",
      "    accuracy                           0.74       555\n",
      "   macro avg       0.64      0.55      0.57       555\n",
      "weighted avg       0.73      0.74      0.73       555\n",
      "\n",
      "----------------------------------------\n",
      "[[285   5  22   1]\n",
      " [ 17  25  30   1]\n",
      " [ 22  17  91   3]\n",
      " [  1   2  24   9]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.6108\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.5838\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24936845904005775),\n",
       " ('LogReg', 0.7153153153153153),\n",
       " ('Linear SVC', 0.7333333333333333),\n",
       " ('SVC', 0.7603603603603604),\n",
       " ('RFC', 0.7387387387387387),\n",
       " ('MNBayes', 0.6108108108108108),\n",
       " ('BerBayes', 0.5837837837837838)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.7117\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24936845904005775),\n",
       " ('LogReg', 0.7153153153153153),\n",
       " ('Linear SVC', 0.7333333333333333),\n",
       " ('SVC', 0.7603603603603604),\n",
       " ('RFC', 0.7387387387387387),\n",
       " ('MNBayes', 0.6108108108108108),\n",
       " ('BerBayes', 0.5837837837837838),\n",
       " ('PassiveAgg', 0.7117117117117117)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "   y_train=='üòä', y_train=='üò©',y_train== 'üò°',y_train=='üò±'\n",
    "]\n",
    "\n",
    "choices = 1 / np.array(list(y_train.value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(n_jobs=-1)\n",
    "xg.fit(train_vec, y_train, eval_metric='mlogloss', sample_weight = np.select(conditions, choices, None))\n",
    "test_preds = xg.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Testing Accuracy: 0.7441\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('XGBoost')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24936845904005775),\n",
       " ('LogReg', 0.7153153153153153),\n",
       " ('Linear SVC', 0.7333333333333333),\n",
       " ('SVC', 0.7603603603603604),\n",
       " ('RFC', 0.7387387387387387),\n",
       " ('MNBayes', 0.6108108108108108),\n",
       " ('BerBayes', 0.5837837837837838),\n",
       " ('PassiveAgg', 0.7117117117117117),\n",
       " ('XGB', 0.7441441441441441)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('XGB', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.90      0.90      0.90       313\n",
      "           üò°       0.47      0.37      0.42        73\n",
      "           üò©       0.59      0.65      0.62       133\n",
      "           üò±       0.49      0.50      0.49        36\n",
      "\n",
      "    accuracy                           0.74       555\n",
      "   macro avg       0.61      0.60      0.61       555\n",
      "weighted avg       0.74      0.74      0.74       555\n",
      "\n",
      "---------------------------------------\n",
      "[[282   8  22   1]\n",
      " [ 14  27  25   7]\n",
      " [ 17  19  86  11]\n",
      " [  2   3  13  18]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('---------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('svc', svc), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('mnbayes', mnb_clf),('berbayes', bb_clf)], #(\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "voting_clf.fit(train_vec, y_train)\n",
    "test_preds = voting_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "Testing Accuracy: 0.7514\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Voting')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Voting', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVC', 0.7603603603603604),\n",
       " ('Voting', 0.7513513513513513),\n",
       " ('XGB', 0.7441441441441441),\n",
       " ('RFC', 0.7387387387387387),\n",
       " ('Linear SVC', 0.7333333333333333),\n",
       " ('LogReg', 0.7153153153153153),\n",
       " ('PassiveAgg', 0.7117117117117117),\n",
       " ('MNBayes', 0.6108108108108108),\n",
       " ('BerBayes', 0.5837837837837838),\n",
       " ('Dummy', 0.24936845904005775)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
