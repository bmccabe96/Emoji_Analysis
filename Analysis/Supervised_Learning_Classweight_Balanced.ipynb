{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries that will be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/brianmccabe/DataScience/Flatiron/mod5/Emoji_Analysis/Scripts/')\n",
    "import config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brianmccabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import string\n",
    "import scipy\n",
    "import emoji\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "seed=42\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_tweet(tweet):\n",
    "    tweet = str(tweet).lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stopwords and punctuations\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords += list(string.punctuation)\n",
    "stopwords += [\"n't\", \"' '\", \"'re'\",\"‚Äù\",\"``\",\"‚Äú\",\"''\",\"‚Äô\",\"'s\",\"'re\",\"http\",\"https\", \"rt\"]\n",
    "alph = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "stopwords += alph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(tweet):\n",
    "    pattern = '((http|https)\\w+\\s\\w+\\s\\w+\\s\\w+)'\n",
    "    try:\n",
    "        return tweet.replace(re.findall(pattern, tweet)[0][0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_percentage(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    cap_count = 0\n",
    "    for item in tokens:\n",
    "        if item.isupper():\n",
    "            cap_count += 1\n",
    "    return cap_count/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_profanity(tweet):\n",
    "    profane = pd.read_csv(\"profane_words.csv\", header=None)\n",
    "\n",
    "    profane = list(profane.loc[:,0])\n",
    "    count = 0\n",
    "    tweet = tweet.lower()\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    for word in tokens:\n",
    "        if word in profane:\n",
    "            count += 1\n",
    "    return count/len(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spelling(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectivity(tweet):\n",
    "    b = TextBlob(tweet)\n",
    "    return b.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(tweet):\n",
    "    try:\n",
    "        p = '[\\w\\s]+(@\\w+)'\n",
    "        return tweet.replace(re.findall(p, tweet)[0], \"\")\n",
    "    except:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(tweet):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(tweet):\n",
    "    tweet = remove_http(tweet)\n",
    "    tweet = remove_username(tweet)\n",
    "    tweet = ReplaceThreeOrMore(tweet)\n",
    "    tokens = process_tweet(tweet)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(tweet):\n",
    "    return analyzer.polarity_scores(tweet)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>-0.2942</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HappyBirthdayKendallJenner now go vote for bid...</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoWayJoe You just tell lies Make promises Make...</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>üò≠</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  I ll kidnap 1000 children before I let this co...          -0.2942   \n",
       "1       Am√©ricaniseUnTitre The Trump Tower Infernale           0.4588   \n",
       "2  HappyBirthdayKendallJenner now go vote for bid...           0.6705   \n",
       "3  Nashies y all know who to vote for @CharlesEst...           0.7424   \n",
       "4  NoWayJoe You just tell lies Make promises Make...           0.4125   \n",
       "\n",
       "  top_emoji  \n",
       "0         üòä  \n",
       "1         üòä  \n",
       "2         üòä  \n",
       "3         üòä  \n",
       "4         üò≠  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweets_4_classes.csv\").drop(['Unnamed: 0', 'emoji_frequency'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ll kidnap 1000 children before I let this company die Mr Waternoose And probably Joe Biden\n",
      "kidnap 100 child let company die mr waternoose probably joe biden\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(df.tweet.iloc[0])\n",
    "print(clean_txt(df.tweet.iloc[0]))\n",
    "print(type(clean_txt(df.tweet.iloc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove \"http link stuff from all the tweets\"\n",
    "# print(df.tweet.iloc[0])\n",
    "# print(remove_http(df.tweet.iloc[0]))\n",
    "\n",
    "# df.tweet = df.tweet.apply(remove_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2507.000000\n",
       "mean        0.486203\n",
       "std         0.325251\n",
       "min         0.000000\n",
       "25%         0.176378\n",
       "50%         0.436967\n",
       "75%         0.826383\n",
       "max         1.000000\n",
       "Name: sentiment_score, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "normalizer = MinMaxScaler()\n",
    "df.sentiment_score = normalizer.fit_transform(np.array(df.sentiment_score).reshape(-1,1))\n",
    "df.sentiment_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2507/2507 [00:00<00:00, 8850.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.731456</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HappyBirthdayKendallJenner now go vote for bid...</td>\n",
       "      <td>0.837699</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.873783</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoWayJoe You just tell lies Make promises Make...</td>\n",
       "      <td>0.708220</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  I ll kidnap 1000 children before I let this co...         0.353558   \n",
       "1       Am√©ricaniseUnTitre The Trump Tower Infernale         0.731456   \n",
       "2  HappyBirthdayKendallJenner now go vote for bid...         0.837699   \n",
       "3  Nashies y all know who to vote for @CharlesEst...         0.873783   \n",
       "4  NoWayJoe You just tell lies Make promises Make...         0.708220   \n",
       "\n",
       "  top_emoji  capitalization  \n",
       "0         üòä        0.117647  \n",
       "1         üòä        0.000000  \n",
       "2         üòä        0.000000  \n",
       "3         üòä        0.000000  \n",
       "4         üò≠        0.000000  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['capitalization'] = df.tweet.progress_apply(capital_percentage)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2507/2507 [00:04<00:00, 515.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>top_emoji</th>\n",
       "      <th>capitalization</th>\n",
       "      <th>profanity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ll kidnap 1000 children before I let this co...</td>\n",
       "      <td>0.353558</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.01087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am√©ricaniseUnTitre The Trump Tower Infernale</td>\n",
       "      <td>0.731456</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HappyBirthdayKendallJenner now go vote for bid...</td>\n",
       "      <td>0.837699</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nashies y all know who to vote for @CharlesEst...</td>\n",
       "      <td>0.873783</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoWayJoe You just tell lies Make promises Make...</td>\n",
       "      <td>0.708220</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment_score  \\\n",
       "0  I ll kidnap 1000 children before I let this co...         0.353558   \n",
       "1       Am√©ricaniseUnTitre The Trump Tower Infernale         0.731456   \n",
       "2  HappyBirthdayKendallJenner now go vote for bid...         0.837699   \n",
       "3  Nashies y all know who to vote for @CharlesEst...         0.873783   \n",
       "4  NoWayJoe You just tell lies Make promises Make...         0.708220   \n",
       "\n",
       "  top_emoji  capitalization  profanity  \n",
       "0         üòä        0.117647    0.01087  \n",
       "1         üòä        0.000000    0.00000  \n",
       "2         üòä        0.000000    0.00000  \n",
       "3         üòä        0.000000    0.00000  \n",
       "4         üò≠        0.000000    0.00800  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profanity'] = df.tweet.progress_apply(check_profanity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2507/2507 [00:00<00:00, 5558.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df['subjectivity'] = df.tweet.progress_apply(get_subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the replacing of extra chars\n",
    "test = \"yoooooo let's gooooo to the zoooo. Wazzzzuppppp. AAABBBCCC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yoo let's goo to the zoo. Wazzupp. AABBCC\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplaceThreeOrMore(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üò≠    1263\n",
       "üòä     947\n",
       "üò°     192\n",
       "üò±     105\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dummy Classifier for Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23374551256481851\n"
     ]
    }
   ],
   "source": [
    "dummy_cf = DummyClassifier(strategy='uniform')\n",
    "dummy_cf.fit(X['tweet'],y)\n",
    "y_preds = dummy_cf.predict(X['tweet'])\n",
    "\n",
    "print(dummy_cf.score(X['tweet'],y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "results.append(('Dummy', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24012764260071798)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üò≠    1263\n",
       "üòä     947\n",
       "üò°     192\n",
       "üò±     105\n",
       "Name: top_emoji, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses balanced, see other notebook for resampled results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# cry = df[df.top_emoji == 'üò≠']\n",
    "# happy = df[df.top_emoji == 'üòä']\n",
    "# fear = df[df.top_emoji == 'üò±']\n",
    "# anger = df[df.top_emoji == 'üò°']\n",
    "\n",
    "\n",
    "# cry_downsampled = resample(cry,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number\n",
    "#                           random_state=seed) \n",
    "\n",
    "# happy_downsampled = resample(happy,\n",
    "#                           replace=False,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# fear_upsampled = resample(fear,\n",
    "#                           replace=True, \n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "# anger_upsampled = resample(anger,\n",
    "#                           replace=True,\n",
    "#                           n_samples=int(len(fear)*1.5), # match number \n",
    "#                           random_state=seed) \n",
    "\n",
    "# df = pd.concat([cry_downsampled, happy_downsampled, fear_upsampled, anger_upsampled])\n",
    "# df.top_emoji.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tweet', 'sentiment_score', 'capitalization', 'profanity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['top_emoji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        \n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        return [{'cap':  row['capitalization'], 'prof': row['profanity'], 'sent': row['sentiment_score']} for _, row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                ('tfidf', TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_txt)),\n",
    "            ])),\n",
    "            \n",
    "            ('embedding', Pipeline([\n",
    "                ('selector', ItemSelector(key='tweet')),\n",
    "                (\"mean_embeddings\", SpacyVectorTransformer(nlp))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['capitalization', 'profanity', 'sentiment_score'])),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 1,#0.9,\n",
    "            'embedding': 1,\n",
    "            'stats': 1 #1.5,\n",
    "        },\n",
    "    ))\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 1) Processing union, total=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('union',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.2,\n",
       "                                                                                  min_df=3,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  preprocessor=<function clean_txt at 0x12095f700>,\n",
       "                                                                                  smooth_idf=1,\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=1,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  use_idf=1))])),\n",
       "                                                ('embedding',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='tweet')),\n",
       "                                                                 ('mean_embeddings',\n",
       "                                                                  SpacyVectorTransformer(nlp=<spacy.lang.en.English object at 0x12647e0d0>))])),\n",
       "                                                ('stats',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['capitalization',\n",
       "                                                                                    'profanity',\n",
       "                                                                                    'sentiment_score'])),\n",
       "                                                                 ('stats',\n",
       "                                                                  TextStats()),\n",
       "                                                                 ('vect',\n",
       "                                                                  DictVectorizer())]))],\n",
       "                              transformer_weights={'stats': 1, 'text': 1}))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the shapes match: (2005, 1479) - (502, 1479)\n",
      "CPU times: user 15.8 s, sys: 61 ms, total: 15.8 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vec = pipeline.transform(X_train)\n",
    "test_vec = pipeline.transform(X_test)\n",
    "print(\"Checking that the shapes match: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty.\n",
    "    - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
    "      not handle warm-starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.5s finished\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2', Cs = [.001,.01,.1,1,10,100], \n",
    "                                    max_iter=10000, verbose=True, n_jobs=-1, scoring='f1', multi_class='ovr',\n",
    "                                class_weight='balanced')\n",
    "lr_clf.fit(train_vec, y_train)\n",
    "test_preds = lr_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "Testing Accuracy: 0.7689\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('LogReg')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('LogReg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.82      0.88      0.85       190\n",
      "           üò°       0.31      0.29      0.30        38\n",
      "           üò≠       0.82      0.80      0.81       253\n",
      "           üò±       0.35      0.29      0.32        21\n",
      "\n",
      "    accuracy                           0.77       502\n",
      "   macro avg       0.58      0.56      0.57       502\n",
      "weighted avg       0.76      0.77      0.77       502\n",
      "\n",
      "----------------------------------------\n",
      "[[167   4  16   3]\n",
      " [  7  11  19   1]\n",
      " [ 28  16 202   7]\n",
      " [  2   5   8   6]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "#Linear Support Vector Machines\n",
    "sv_clf = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=seed,verbose=3) \n",
    "sv_clf.fit(train_vec, y_train)\n",
    "test_preds = sv_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Testing Accuracy: 0.7809\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Linear SVC')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Linear SVC', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[.1,1,10,100],'gamma':[100,10,1,0.1,0.001], 'kernel':['linear','rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   49.7s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100],\n",
       "                         'gamma': [100, 10, 1, 0.1, 0.001],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc=GridSearchCV(SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_svc.fit(train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7910174129353235\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "SVC(C=100, gamma=0.001)\n"
     ]
    }
   ],
   "source": [
    "print(grid_svc.best_score_)\n",
    "print(grid_svc.best_params_)\n",
    "print(grid_svc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = grid_svc.best_estimator_.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Grid\n",
      "Testing Accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('SVC Grid')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24012764260071798),\n",
       " ('LogReg', 0.7689243027888446),\n",
       " ('Linear SVC', 0.7808764940239044),\n",
       " ('SVC_Grid', 0.8027888446215139)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.append(('SVC_Grid', accuracy))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_clf = RandomForestClassifier(n_estimators=250, random_state=seed,n_jobs=-1,verbose=1, class_weight='balanced')\n",
    "rfc_clf.fit(train_vec, y_train)\n",
    "test_preds = rfc_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Testing Accuracy: 0.7928\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Random Forest')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('RFC', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.86      0.87      0.87       190\n",
      "           üò°       0.00      0.00      0.00        38\n",
      "           üò≠       0.75      0.92      0.83       253\n",
      "           üò±       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.79       502\n",
      "   macro avg       0.40      0.45      0.42       502\n",
      "weighted avg       0.70      0.79      0.74       502\n",
      "\n",
      "----------------------------------------\n",
      "[[165   0  25   0]\n",
      " [  5   0  33   0]\n",
      " [ 20   0 233   0]\n",
      " [  1   0  20   0]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('----------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-337770b34a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Multinomial Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "mnb_clf = MultinomialNB() \n",
    "mnb_clf.fit(train_vec, y_train)\n",
    "test_preds = mnb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('MNBayes', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bernoulli Naive Bayes\n",
    "bb_clf = BernoulliNB() \n",
    "bb_clf.fit(train_vec, y_train)\n",
    "test_preds = bb_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Bayes\n",
      "Testing Accuracy: 0.5936\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Bernoulli Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('BerBayes', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24012764260071798),\n",
       " ('LogReg', 0.7689243027888446),\n",
       " ('Linear SVC', 0.7808764940239044),\n",
       " ('SVC_Grid', 0.8027888446215139),\n",
       " ('RFC', 0.7928286852589641),\n",
       " ('BerBayes', 0.5936254980079682)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PassiveAggresive Classifier\n",
    "pac_clf = PassiveAggressiveClassifier() \n",
    "pac_clf.fit(train_vec, y_train)\n",
    "test_preds = pac_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Bayes\n",
      "Testing Accuracy: 0.7729\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('MN Bayes')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('PassiveAgg', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24012764260071798),\n",
       " ('LogReg', 0.7689243027888446),\n",
       " ('Linear SVC', 0.7808764940239044),\n",
       " ('SVC_Grid', 0.8027888446215139),\n",
       " ('RFC', 0.7928286852589641),\n",
       " ('BerBayes', 0.5936254980079682),\n",
       " ('PassiveAgg', 0.7729083665338645)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN A BASELINE XGB\n",
    "xg = xgb.XGBClassifier()\n",
    "xg.fit(train_vec, y_train)\n",
    "test_preds = xg.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Testing Accuracy: 0.8008\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('XGBoost')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('XGB', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dummy', 0.24012764260071798),\n",
       " ('LogReg', 0.7689243027888446),\n",
       " ('Linear SVC', 0.7808764940239044),\n",
       " ('SVC_Grid', 0.8027888446215139),\n",
       " ('RFC', 0.7928286852589641),\n",
       " ('BerBayes', 0.5936254980079682),\n",
       " ('PassiveAgg', 0.7729083665338645),\n",
       " ('XGB', 0.8007968127490039)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           üòä       0.84      0.91      0.87       190\n",
      "           üò°       0.80      0.11      0.19        38\n",
      "           üò≠       0.78      0.89      0.83       253\n",
      "           üò±       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.80       502\n",
      "   macro avg       0.60      0.48      0.47       502\n",
      "weighted avg       0.77      0.80      0.76       502\n",
      "\n",
      "---------------------------------------\n",
      "[[173   0  17   0]\n",
      " [  5   4  29   0]\n",
      " [ 27   1 225   0]\n",
      " [  2   0  19   0]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))\n",
    "print('---------------------------------------')\n",
    "print(confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 250 out of 250 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr_clf), ('svm_linear', sv_clf), ('pass_aggr', pac_clf),\n",
    "                            ('smv_grid', grid_svc.best_estimator_), ('xgboost', xg), ('rfc', rfc_clf),\n",
    "                            ('berbayes', bb_clf)], #('mnbayes', mnb_clf),\n",
    "                voting='hard', verbose=1, n_jobs= -1)\n",
    "voting_clf.fit(train_vec, y_train)\n",
    "test_preds = voting_clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting\n",
      "Testing Accuracy: 0.8048\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print('Voting')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy))\n",
    "\n",
    "results.append(('Voting', accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Voting', 0.8047808764940239),\n",
       " ('SVC_Grid', 0.8027888446215139),\n",
       " ('XGB', 0.8007968127490039),\n",
       " ('RFC', 0.7928286852589641),\n",
       " ('Linear SVC', 0.7808764940239044),\n",
       " ('PassiveAgg', 0.7729083665338645),\n",
       " ('LogReg', 0.7689243027888446),\n",
       " ('BerBayes', 0.5936254980079682),\n",
       " ('Dummy', 0.24012764260071798)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart of Different Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x[0] for x in results]\n",
    "y = [x[1] for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Early Results')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x,y)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Early Results\", fontsize=20)\n",
    "#plt.savefig(\"../pics/model_performances.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
